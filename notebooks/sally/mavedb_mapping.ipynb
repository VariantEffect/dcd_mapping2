{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e66de94",
   "metadata": {},
   "source": [
    "# MaveDB Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9dce63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Required Packages\n",
    "import io\n",
    "import re\n",
    "import requests\n",
    "import hgvs\n",
    "import base64, hashlib\n",
    "from ga4gh.vrs import models, vrs_deref, vrs_enref\n",
    "from ga4gh.core import ga4gh_identify, ga4gh_serialize, ga4gh_digest, ga4gh_deref, sha512t24u\n",
    "from ga4gh.vrs.extras.translator import AlleleTranslator\n",
    "from ga4gh.vrs.dataproxy import SeqRepoDataProxy\n",
    "from ga4gh.vrs.normalize import normalize\n",
    "import pandas as pd\n",
    "from gene.query import QueryHandler\n",
    "from gene.database import create_db\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "#from cool_seq_tool.cool_seq_tool import CoolSeqTool\n",
    "#from cool_seq_tool.data_sources.uta_database import UTADatabase\n",
    "from cool_seq_tool.sources.uta_database import UtaDatabase as UTADatabase # use alias for back-compatibility with the rest of the notebook\n",
    "#from cool_seq_tool.data_sources.mane_transcript_mappings import MANETranscriptMappings\n",
    "from cool_seq_tool.sources.mane_transcript_mappings import ManeTranscriptMappings as MANETranscriptMappings # use alias for back-compatibility with the rest of the notebook\n",
    "import pickle\n",
    "from os import environ\n",
    "import Bio\n",
    "from Bio.SeqUtils import seq1\n",
    "from Bio.Seq import Seq\n",
    "from biocommons.seqrepo import SeqRepo\n",
    "from bs4 import BeautifulSoup\n",
    "sr = SeqRepo(\"/usr/local/share/seqrepo/2021-01-29\", writeable = True)\n",
    "environ[\"UTA_DB_URL\"] = 'postgresql://anonymous:anonymous@uta.biocommons.org:5432/uta'\n",
    "environ[\"GENE_NORM_DB_URL\"] = 'postgres://postgres:postgres@localhost:5432/gene_normalizer'\n",
    "#from pyliftover import LiftOver\n",
    "import subprocess\n",
    "import mavehgvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98fdd2",
   "metadata": {},
   "source": [
    "# The blocks below can be run to reproduce the output in the results directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d05229",
   "metadata": {},
   "source": [
    "## Process Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caa00b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied with slight mod from src/dcd_mapping/resources.py\n",
    "def _get_uniprot_ref(scoreset_json):\n",
    "    \"\"\"Extract UniProt reference from scoreset metadata if available.\n",
    "\n",
    "    :param scoreset_json: parsed JSON from scoresets API\n",
    "    :return: UniProt ID if available\n",
    "    \"\"\"\n",
    "    ext_ids = scoreset_json[\"targetGenes\"][0].get(\"externalIdentifiers\")\n",
    "    if not ext_ids:\n",
    "        return None\n",
    "    for ext_id in ext_ids:\n",
    "        if ext_id.get(\"identifier\", {}).get(\"dbName\") == \"UniProt\":\n",
    "            return f\"uniprot:{ext_id['identifier']['identifier']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "25a899bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this replaces the several blocks above\n",
    "# only get metadata for a subset of scoresets\n",
    "# and use new API documentation to parse the json responses\n",
    "\n",
    "urns = list()\n",
    "target_sequences = list()\n",
    "target_sequence_type = list()\n",
    "targets = list()\n",
    "assembly = list()\n",
    "uniprot = list()\n",
    "target_type = list()\n",
    "\n",
    "scoreset_urns = ['urn:mavedb:00000041-a-1',\n",
    "                 'urn:mavedb:00000048-a-1',\n",
    "                 'urn:mavedb:00000068-b-1,'\n",
    "                 'urn:mavedb:00000045-c-1',\n",
    "                 'urn:mavedb:00000018-a-1',\n",
    "                 'urn:mavedb:00000107-a-1',\n",
    "                 'urn:mavedb:00000103-d-1',\n",
    "                 'urn:mavedb:00000029-a-2',\n",
    "                 'urn:mavedb:00000061-b-1',\n",
    "                 'urn:mavedb:00000097-q-1',\n",
    "                 'urn:mavedb:00000003-a-1',\n",
    "                 'urn:mavedb:00000097-i-1',\n",
    "                 'urn:mavedb:00000099-a-1'\n",
    "                 ]\n",
    "for scoreset_urn in scoreset_urns:\n",
    "    url = f\"https://api.mavedb.org/api/v1/score-sets/{scoreset_urn}\"\n",
    "    response = requests.get(url)\n",
    "    json_parse = response.json()\n",
    "    if 'targetGenes' in json_parse:\n",
    "        if len(json_parse['targetGenes']) == 1:\n",
    "            if json_parse['targetGenes'][0]['targetSequence']['reference']['organismName'] == 'Homo sapiens':\n",
    "                urns.append(json_parse['urn'])\n",
    "                target_sequences.append(json_parse['targetGenes'][0]['targetSequence']['sequence'])\n",
    "                target_sequence_type.append(json_parse['targetGenes'][0]['targetSequence']['sequenceType'])\n",
    "                targets.append(json_parse['targetGenes'][0]['name'])\n",
    "                assembly.append(json_parse['targetGenes'][0]['targetSequence']['reference']['shortName'])\n",
    "                uniprot.append(_get_uniprot_ref(json_parse))\n",
    "                target_type.append(json_parse['targetGenes'][0]['category'])\n",
    "\n",
    "# Create, save dataframe\n",
    "dat = {'urn': urns, 'target_sequence': target_sequences, 'target_sequence_type': target_sequence_type, 'target':targets, \n",
    "       'assembly_id':assembly, 'uniprot_id':uniprot, 'target_type':target_type}\n",
    "dat = pd.DataFrame(data=dat)\n",
    "dat.to_csv('mave_dat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00b98f",
   "metadata": {},
   "source": [
    "## Part 1: MaveDB Metadata to BLAT Alignment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "436f661c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urn</th>\n",
       "      <th>target_sequence</th>\n",
       "      <th>target_sequence_type</th>\n",
       "      <th>target</th>\n",
       "      <th>assembly_id</th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>target_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>urn:mavedb:00000041-a-1</td>\n",
       "      <td>CTGCGGCTGGAGGTCAAGCTGGGCCAGGGCTGCTTTGGCGAGGTGT...</td>\n",
       "      <td>dna</td>\n",
       "      <td>Src catalytic domain</td>\n",
       "      <td>hg38</td>\n",
       "      <td>uniprot:P12931</td>\n",
       "      <td>Protein coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>urn:mavedb:00000048-a-1</td>\n",
       "      <td>GAGGGGATCAGTATATACACTTCAGATAACTACACCGAGGAAATGG...</td>\n",
       "      <td>dna</td>\n",
       "      <td>CXCR4</td>\n",
       "      <td>hg38</td>\n",
       "      <td>uniprot:P61073</td>\n",
       "      <td>Protein coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>urn:mavedb:00000018-a-1</td>\n",
       "      <td>GGTGTCTGTTTGAGGTTGCTAGTGAACACAGTTGTGTCAGAAGCAA...</td>\n",
       "      <td>dna</td>\n",
       "      <td>HBB promoter</td>\n",
       "      <td>hg38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Regulatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>urn:mavedb:00000107-a-1</td>\n",
       "      <td>MDAPRQVVNFGPGPAKLPHSVLLEIQKELLDYKGVGISVLEMSHRS...</td>\n",
       "      <td>protein</td>\n",
       "      <td>PSAT1</td>\n",
       "      <td>hg38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Protein coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>urn:mavedb:00000103-d-1</td>\n",
       "      <td>MAAAAAAGAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNV...</td>\n",
       "      <td>protein</td>\n",
       "      <td>MAPK1</td>\n",
       "      <td>hg38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Protein coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>urn:mavedb:00000029-a-2</td>\n",
       "      <td>GAACTGGAAAAGCCCTGTCCGGTGAGGGGGCAGAAGGACTCAGCGC...</td>\n",
       "      <td>dna</td>\n",
       "      <td>SORT1 enhancer</td>\n",
       "      <td>hg38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Regulatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>urn:mavedb:00000061-b-1</td>\n",
       "      <td>TCTAAGACAAGCAACACTATCCGTGTTTTCTTGCCGAACAAGCAAA...</td>\n",
       "      <td>dna</td>\n",
       "      <td>RAF</td>\n",
       "      <td>hg38</td>\n",
       "      <td>uniprot:P04049</td>\n",
       "      <td>Protein coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>urn:mavedb:00000097-q-1</td>\n",
       "      <td>TTTCTTTCAGCATGATTTTGAAGTCAGAGGAGATGTGGTCAATGGA...</td>\n",
       "      <td>dna</td>\n",
       "      <td>BRCA1 Exon 19</td>\n",
       "      <td>hg19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Protein coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>urn:mavedb:00000003-a-1</td>\n",
       "      <td>GATTTATCTGCTCTTCGCGTTGAAGAAGTACAAAATGTCATTAATG...</td>\n",
       "      <td>dna</td>\n",
       "      <td>BRCA1 RING domain</td>\n",
       "      <td>hg38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Protein coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>urn:mavedb:00000097-i-1</td>\n",
       "      <td>AGTGTGAGCAGGGAGAAGCCAGAATTGACAGCTTCAACAGAAAGGG...</td>\n",
       "      <td>dna</td>\n",
       "      <td>BRCA1 Exon 15</td>\n",
       "      <td>hg19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Protein coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>urn:mavedb:00000099-a-1</td>\n",
       "      <td>ATGAATGGCACAGAAGGCCCTAACTTCTACGTGCCCTTCTCCAATG...</td>\n",
       "      <td>dna</td>\n",
       "      <td>RHO</td>\n",
       "      <td>hg38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Protein coding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        urn  \\\n",
       "0   urn:mavedb:00000041-a-1   \n",
       "1   urn:mavedb:00000048-a-1   \n",
       "2   urn:mavedb:00000018-a-1   \n",
       "3   urn:mavedb:00000107-a-1   \n",
       "4   urn:mavedb:00000103-d-1   \n",
       "5   urn:mavedb:00000029-a-2   \n",
       "6   urn:mavedb:00000061-b-1   \n",
       "7   urn:mavedb:00000097-q-1   \n",
       "8   urn:mavedb:00000003-a-1   \n",
       "9   urn:mavedb:00000097-i-1   \n",
       "10  urn:mavedb:00000099-a-1   \n",
       "\n",
       "                                      target_sequence target_sequence_type  \\\n",
       "0   CTGCGGCTGGAGGTCAAGCTGGGCCAGGGCTGCTTTGGCGAGGTGT...                  dna   \n",
       "1   GAGGGGATCAGTATATACACTTCAGATAACTACACCGAGGAAATGG...                  dna   \n",
       "2   GGTGTCTGTTTGAGGTTGCTAGTGAACACAGTTGTGTCAGAAGCAA...                  dna   \n",
       "3   MDAPRQVVNFGPGPAKLPHSVLLEIQKELLDYKGVGISVLEMSHRS...              protein   \n",
       "4   MAAAAAAGAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNV...              protein   \n",
       "5   GAACTGGAAAAGCCCTGTCCGGTGAGGGGGCAGAAGGACTCAGCGC...                  dna   \n",
       "6   TCTAAGACAAGCAACACTATCCGTGTTTTCTTGCCGAACAAGCAAA...                  dna   \n",
       "7   TTTCTTTCAGCATGATTTTGAAGTCAGAGGAGATGTGGTCAATGGA...                  dna   \n",
       "8   GATTTATCTGCTCTTCGCGTTGAAGAAGTACAAAATGTCATTAATG...                  dna   \n",
       "9   AGTGTGAGCAGGGAGAAGCCAGAATTGACAGCTTCAACAGAAAGGG...                  dna   \n",
       "10  ATGAATGGCACAGAAGGCCCTAACTTCTACGTGCCCTTCTCCAATG...                  dna   \n",
       "\n",
       "                  target assembly_id      uniprot_id     target_type  \n",
       "0   Src catalytic domain        hg38  uniprot:P12931  Protein coding  \n",
       "1                  CXCR4        hg38  uniprot:P61073  Protein coding  \n",
       "2           HBB promoter        hg38             NaN      Regulatory  \n",
       "3                  PSAT1        hg38             NaN  Protein coding  \n",
       "4                  MAPK1        hg38             NaN  Protein coding  \n",
       "5         SORT1 enhancer        hg38             NaN      Regulatory  \n",
       "6                    RAF        hg38  uniprot:P04049  Protein coding  \n",
       "7          BRCA1 Exon 19        hg19             NaN  Protein coding  \n",
       "8      BRCA1 RING domain        hg38             NaN  Protein coding  \n",
       "9          BRCA1 Exon 15        hg19             NaN  Protein coding  \n",
       "10                   RHO        hg38             NaN  Protein coding  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = pd.read_csv('mave_dat.csv', index_col=0)\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2de5a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment Helper Function\n",
    "def get_gene_data(i, blat_chr, return_chr):\n",
    "    qh = QueryHandler(create_db())\n",
    "    try:\n",
    "        uniprot = dat.at[i,'uniprot_id']\n",
    "        gsymb = qh.normalize(str(f'uniprot:{uniprot}')).gene_descriptor.label\n",
    "    except:\n",
    "        try:\n",
    "            target = dat.at[i, 'target'].split(' ')[0]\n",
    "            gsymb = qh.normalize(target).gene_descriptor.label\n",
    "        except:\n",
    "            return 'NA' # if gsymb cannot be extracted\n",
    "   \n",
    "    temp = qh.search(gsymb).source_matches\n",
    "    source_dict = {}\n",
    "    for i in range(len(temp)):\n",
    "        source_dict[temp[i].source] = i\n",
    "    \n",
    "    if 'HGNC' in source_dict and return_chr == True:\n",
    "        chrom = temp[source_dict['HGNC']].records[0].locations[0].chr\n",
    "        return chrom\n",
    "    \n",
    "    if 'Ensembl' in source_dict and return_chr == False and len(temp[source_dict['Ensembl']].records) != 0:\n",
    "        for j in range(len(temp[source_dict['Ensembl']].records)):\n",
    "            for k in range(len(temp[source_dict['Ensembl']].records[j].locations)):\n",
    "                if temp[source_dict['Ensembl']].records[j].locations[k].interval.type == 'SequenceInterval': # Multiple records per source\n",
    "                    start = temp[source_dict['Ensembl']].records[j].locations[k].interval.start.value\n",
    "                    end = temp[source_dict['Ensembl']].records[j].locations[k].interval.end.value\n",
    "                    loc_list = {}\n",
    "                    loc_list['start'] = start\n",
    "                    loc_list['end'] = end\n",
    "                    return loc_list\n",
    "    if 'NCBI' in source_dict and return_chr == False and len(temp[source_dict['NCBI']].records) != 0:\n",
    "        for j in range(len(temp[source_dict['NCBI']].records)):\n",
    "            for k in range(len(temp[source_dict['NCBI']].records[j].locations)):\n",
    "                if temp[source_dict['NCBI']].records[j].locations[k].interval.type == 'SequenceInterval':\n",
    "                    start = temp[source_dict['NCBI']].records[j].locations[k].interval.start.value\n",
    "                    end = temp[source_dict['NCBI']].records[j].locations[k].interval.end.value\n",
    "                    loc_list = {}\n",
    "                    loc_list['start'] = start\n",
    "                    loc_list['end'] = end\n",
    "                    return loc_list\n",
    "    return 'NA'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b52e25b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: RHO\n",
      "       <unknown description>\n",
      "  Hit: chr3 (198295559)\n",
      "       <unknown description>\n",
      " HSPs: ----  --------  ---------  ------  ---------------  ---------------------\n",
      "          #   E-value  Bit score    Span      Query range              Hit range\n",
      "       ----  --------  ---------  ------  ---------------  ---------------------\n",
      "          0         ?          ?       ?         [0:1047]  [129528733:129533718]\n",
      "129528733\n"
     ]
    }
   ],
   "source": [
    "# playing around with blat results\n",
    "from Bio import SearchIO\n",
    "result = SearchIO.read('blat_out.psl', 'blat-psl')\n",
    "print(result[0])\n",
    "print(result[0][0].hit_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ae862b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urn:mavedb:00000041-a-1\n",
      "Loaded 3209286105 letters in 455 sequences\n",
      "Searched 750 bases in 1 sequences\n",
      "added urn:mavedb:00000041-a-1 to dict\n",
      "urn:mavedb:00000048-a-1\n",
      "Loaded 3209286105 letters in 455 sequences\n",
      "Searched 1053 bases in 1 sequences\n",
      "added urn:mavedb:00000048-a-1 to dict\n",
      "urn:mavedb:00000018-a-1\n",
      "Loaded 3209286105 letters in 455 sequences\n",
      "Searched 187 bases in 1 sequences\n",
      "added urn:mavedb:00000018-a-1 to dict\n",
      "urn:mavedb:00000107-a-1\n",
      "Loaded 3209286105 letters in 455 sequences\n",
      "Blatx 455 sequences in database, 1 files in query\n",
      "added urn:mavedb:00000107-a-1 to dict\n",
      "urn:mavedb:00000103-d-1\n",
      "Loaded 3209286105 letters in 455 sequences\n",
      "Blatx 455 sequences in database, 1 files in query\n",
      "added urn:mavedb:00000103-d-1 to dict\n",
      "urn:mavedb:00000029-a-2\n",
      "Loaded 3209286105 letters in 455 sequences\n",
      "Searched 600 bases in 1 sequences\n",
      "added urn:mavedb:00000029-a-2 to dict\n",
      "urn:mavedb:00000061-b-1\n",
      "Loaded 3209286105 letters in 455 sequences\n",
      "Searched 117 bases in 1 sequences\n",
      "added urn:mavedb:00000061-b-1 to dict\n",
      "urn:mavedb:00000097-q-1\n",
      "Loaded 3209286105 letters in 455 sequences\n",
      "Searched 104 bases in 1 sequences\n",
      "added urn:mavedb:00000097-q-1 to dict\n",
      "urn:mavedb:00000003-a-1\n",
      "Loaded 3209286105 letters in 455 sequences\n",
      "Searched 909 bases in 1 sequences\n",
      "added urn:mavedb:00000003-a-1 to dict\n",
      "urn:mavedb:00000097-i-1\n",
      "Loaded 3209286105 letters in 455 sequences\n",
      "Searched 106 bases in 1 sequences\n",
      "added urn:mavedb:00000097-i-1 to dict\n",
      "urn:mavedb:00000099-a-1\n",
      "Loaded 3209286105 letters in 455 sequences\n",
      "Searched 1047 bases in 1 sequences\n",
      "added urn:mavedb:00000099-a-1 to dict\n"
     ]
    }
   ],
   "source": [
    "# Get Query and Hit Ranges for Each Human Target Sequence\n",
    "from Bio import SearchIO\n",
    "mave_blat_dict = {}\n",
    "blat_exec_path = '/Users/sallybg/workspace/blat/bin/blat'\n",
    "blat_ref_path = '/Users/sallybg/workspace/blat/hg38.2bit'\n",
    "\n",
    "for i in range(len(dat.index)):\n",
    "    print(dat.at[i, 'urn'])\n",
    "    blat_file = open('blat_query.fa', 'w')\n",
    "    blat_file.write('>' + dat.at[i, 'target'] + '\\n')\n",
    "    blat_file.write(dat.at[i, 'target_sequence'] + '\\n')\n",
    "    blat_file.close()\n",
    "\n",
    "    if dat.at[i, 'target_sequence_type'] == 'protein':\n",
    "        subprocess.run([blat_exec_path, blat_ref_path, '-q=prot', '-t=dnax', '-minScore=20', 'blat_query.fa', 'blat_out.psl'])\n",
    "    else:\n",
    "        subprocess.run([blat_exec_path, blat_ref_path, '-minScore=20', 'blat_query.fa', 'blat_out.psl'])\n",
    "\n",
    "    # Extract ranges\n",
    "    chrom = ''\n",
    "    strand = ''\n",
    "    target = ''\n",
    "    target_type = ''\n",
    "    coverage = None\n",
    "    identity = None\n",
    "    query_ranges = list()\n",
    "    hit_ranges = list()\n",
    "    \n",
    "    try:\n",
    "        output = SearchIO.read('blat_out.psl', 'blat-psl')\n",
    "    except:\n",
    "        try:\n",
    "            subprocess.run([blat_exec_path, blat_ref_path, '-q=dnax', '-t=dnax', '-minScore=20', 'blat_query.fa', 'blat_out.psl'])\n",
    "            output = SearchIO.read('blat_out.psl', 'blat-psl')\n",
    "        except:\n",
    "            qh_dat = {'query_ranges': list('NA'), 'hit_ranges': list('NA')}\n",
    "            qh_dat = pd.DataFrame(data = qh_dat)\n",
    "            mave_blat_dict[dat.at[i, 'urn']] = {'chrom': 'NA', 'strand': 'NA', 'target': 'NA', 'target_type': 'NA',\n",
    "                                            'uniprot': 'NA','coverage': 'NA','identity':'NA', 'hits': qh_dat}\n",
    "            continue\n",
    "\n",
    "    # Find chromosome to select hit from\n",
    "    hit_scores = list()\n",
    "    hit_dict = {}\n",
    "    use_chr = False\n",
    "    \n",
    "    for c in range(len(output)):\n",
    "        correct_chr = get_gene_data(i,output[c].id.strip('chr'), return_chr = True)\n",
    "        if correct_chr == output[c].id.strip('chr'):\n",
    "            use_chr = True\n",
    "            break\n",
    "        if correct_chr == 'NA': # Take top scoring hit if target not found using gene normalizer\n",
    "            hit_scores = list()\n",
    "            for e in range(len(output[c])):\n",
    "                hit_scores.append(output[c][e].score)\n",
    "            hit_dict[c] = hit_scores\n",
    "\n",
    "    if use_chr == False:\n",
    "        for key in hit_dict:\n",
    "            hit_dict[key] = max(hit_dict[key])\n",
    "        hit = max(hit_dict, key = hit_dict.get)\n",
    "    else:\n",
    "        hit = c\n",
    "                             \n",
    "    \n",
    "    # Use location provided by gene normalizer to find hsp\n",
    "    loc_dict = get_gene_data(i, output[hit].id.strip('chr'), return_chr = False)\n",
    "    \n",
    "    hit_starts = list()\n",
    "    for n in range(len(output[hit])):\n",
    "        hit_starts.append(output[hit][n].hit_start)\n",
    "    \n",
    "    sub_scores = list()\n",
    "    for n in range(len(output[hit])):\n",
    "        sub_scores.append(output[hit][n].score)\n",
    "    \n",
    "    if loc_dict == 'NA':\n",
    "        hsp = output[hit][sub_scores.index(max(sub_scores))] # Take top score if no match found \n",
    "    else:\n",
    "        hsp = output[hit][hit_starts.index(min(hit_starts, key=lambda x:abs(x - loc_dict['start'])))]\n",
    "\n",
    "        \n",
    "    for j in range(len(hsp)):\n",
    "        test_file = open('blat_output_test.txt', 'w')\n",
    "        test_file.write(str(hsp[j]))\n",
    "        test_file.close()\n",
    "\n",
    "        query_string = ''\n",
    "        hit_string = ''\n",
    "        strand = hsp[0].query_strand\n",
    "        coverage = 100 * (hsp.query_end - hsp.query_start) / output.seq_len\n",
    "        coverage = f\"{hsp.query_end - hsp.query_start} / {output.seq_len}, {coverage}\" \n",
    "        identity = hsp.ident_pct\n",
    "\n",
    "        test_file = open('blat_output_test.txt', 'r')\n",
    "        for k,line in enumerate(test_file):\n",
    "            if k == 1:\n",
    "                chrom = line.strip('\\n')\n",
    "            if k == 2:\n",
    "                query_string = line.strip('\\n')\n",
    "            if k == 3:\n",
    "                hit_string = line.strip('\\n')\n",
    "        test_file.close()\n",
    "\n",
    "        chrom = chrom.split(' ')[9].strip('chr')\n",
    "        query_string = query_string.split(' ')\n",
    "        hit_string = hit_string.split(' ')\n",
    "        query_ranges.append(query_string[2])\n",
    "        hit_ranges.append(hit_string[4])\n",
    "        \n",
    "    # Add to dict\n",
    "    qh_dat = {'query_ranges': query_ranges, 'hit_ranges': hit_ranges}\n",
    "    qh_dat = pd.DataFrame(data = qh_dat)\n",
    "    mave_blat_dict[dat.at[i, 'urn']] = {'chrom': chrom,'strand': strand,'target': dat.at[i,'target'], 'target_type': dat.at[i, 'target_type'],\n",
    "                                        'uniprot': dat.at[i,'uniprot_id'],'coverage': coverage,'identity': identity, 'hits': qh_dat} \n",
    "    print('added ' + dat.at[i, 'urn'] + ' to dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d66f6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mave_blat.pickle', 'wb') as fn:\n",
    "    pickle.dump(mave_blat_dict, fn, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad39211",
   "metadata": {},
   "source": [
    "## Part 2: BLAT Output to Transcript Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1059d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "def get_start(string):\n",
    "    return int(string.split(':')[0].strip('['))\n",
    "\n",
    "def get_end(string):\n",
    "    return int(string.split(':')[1].strip(']'))\n",
    "\n",
    "def get_locs_list(hitsdat):\n",
    "    locs_list = []\n",
    "    for i in range(len(hitsdat.index)):\n",
    "        start = get_start(hitsdat.at[i, 'hit_ranges'])\n",
    "        end = get_end(hitsdat.at[i, 'hit_ranges'])\n",
    "        locs_list.append([start,end])\n",
    "    return locs_list\n",
    "\n",
    "def get_hits_list(hitsdat):\n",
    "    hits_list = []\n",
    "    for i in range(len(hitsdat.index)):\n",
    "        start = get_start(hitsdat.at[i, 'query_ranges'])\n",
    "        end = get_end(hitsdat.at[i, 'query_ranges'])\n",
    "        hits_list.append([start,end])\n",
    "    return hits_list\n",
    "\n",
    "def get_query_hits(dat):\n",
    "    query_list = []\n",
    "    hits_list = []\n",
    "    for i in range(len(dat.index)):\n",
    "        query_start = get_start(dat.at[i, 'query_ranges'])\n",
    "        query_end = get_end(dat.at[i, 'query_ranges'])\n",
    "        query_list.append([query_start, query_end])\n",
    "        hit_start = get_start(dat.at[i, 'hit_ranges'])\n",
    "        hit_end = get_end(dat.at[i, 'hit_ranges'])\n",
    "        hits_list.append([hit_start, hit_end])\n",
    "        return query_list, hits_list\n",
    "\n",
    "def get_ga4gh(dp, ref):\n",
    "    aliases = dp.get_metadata(ref)['aliases']\n",
    "    f = filter(lambda x: 'ga4gh' in x, aliases)\n",
    "    return 'ga4gh:' + list(f)[0].split(':')[1]\n",
    "\n",
    "def get_chr(dp, chrom):\n",
    "    aliases = dp.get_metadata('GRCh38:' + chrom)['aliases']\n",
    "    f = filter(lambda x: 'refseq' in x, aliases)\n",
    "    return list(f)[0].split(':')[1]\n",
    "\n",
    "def modify_hgvs(var, ref, off, hp):\n",
    "    if len(var) == 3 or var == '_wt' or var == '_sy' or '[' in var:\n",
    "        return var\n",
    "    var = ref + ':' + var\n",
    "    var = hp.parse_hgvs_variant(var)\n",
    "    var.posedit.pos.start.base = var.posedit.pos.start.base + off\n",
    "    var.posedit.pos.end.base = var.posedit.pos.end.base + off\n",
    "    return(str(var))\n",
    "\n",
    "def blat_check(i):\n",
    "    item = mave_blat_dict[dat.at[i, 'urn']]\n",
    "    if item['uniprot'] == None:\n",
    "        test = dat.at[i, 'target'].split(' ')\n",
    "        for j in range(len(test)):\n",
    "            try:\n",
    "                out = qh.normalize(test[j]).gene\n",
    "                gene_dat = [out.label, out.extensions[2].value['chr']]\n",
    "                if item['chrom'] != gene_dat[1]:\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "def get_haplotype_allele(var, ref, offset, l, tr, dp, ts, mapped, ranges, hits, strand):\n",
    "    var = var.lstrip(f'{l}.')\n",
    "\n",
    "    if '[' in var:\n",
    "        var = var[1:][:-1]\n",
    "        varlist = var.split(';')\n",
    "        varlist = list(set(varlist))\n",
    "    else:\n",
    "        varlist = list()\n",
    "        varlist.append(var)\n",
    "\n",
    "    locs = {}\n",
    "    alleles = []\n",
    "\n",
    "    for i in range(len(varlist)):\n",
    "        try:\n",
    "            hgvs_string = ref + ':'+ l +'.' + varlist[i]\n",
    "            allele = tr.translate_from(hgvs_string, 'hgvs')\n",
    "            \n",
    "            if mapped == 'pre':\n",
    "                print(allele)\n",
    "                allele.location.sequence_id = 'ga4gh:SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "                if 'dup' in hgvs_string:\n",
    "                    allele.state.sequence = 2*str(sr[str(allele.location.sequence_id)][allele.location.start.value:allele.location.end.value])\n",
    "                    \n",
    "            else:\n",
    "                if l != 'g':\n",
    "                    allele.location.start.value = allele.location.start.value + offset\n",
    "                    allele.location.end.value = allele.location.end.value + offset\n",
    "                    if 'dup' in hgvs_string:\n",
    "                        allele.state.sequence = 2*str(sr[str(allele.location.sequence_id)][allele.location.start.value:allele.location.end.value])\n",
    "                        \n",
    "                else:\n",
    "                    start = allele.location.start.value\n",
    "                    if len(hits) == 1 and strand == 1:\n",
    "                        i = 0\n",
    "                        diff = start - hits[i][0]\n",
    "                        diff2 = allele.location.end.value - start\n",
    "                        allele.location.start.value = ranges[i][0] + diff\n",
    "                        allele.location.end.value = allele.location.start.value + diff2\n",
    "                    else:\n",
    "                        for i in range(len(hits)):\n",
    "                            if start >= hits[i][0] and start < hits[i][1]:\n",
    "                                break\n",
    "                        diff = start - hits[i][0]\n",
    "                        diff2 = allele.location.end.value - start\n",
    "                        if strand == 1: # positive orientation\n",
    "                            allele.location.start.value = ranges[i][0] + diff\n",
    "                            allele.location.end.value = allele.location.start.value + diff2\n",
    "                            if 'dup' in hgvs_string:\n",
    "                                allele.state.sequence = 2*str(sr[str(allele.location.sequence_id)][allele.location.start.value:allele.location.end.value])\n",
    "                        else: \n",
    "                            allele.location.start.value = ranges[i][1] - diff - diff2\n",
    "                            allele.location.end.value = allele.location.start.value + diff2\n",
    "                            if 'dup' in hgvs_string:\n",
    "                                allele.state.sequence = 2*str(sr[str(allele.location.sequence_id)][allele.location.start.value:allele.location.end.value])\n",
    "                            allele.state.sequence = str(Seq(str(allele.state.sequence)).reverse_complement())\n",
    "            \n",
    "            if allele.state.sequence == 'N' and l != 'p':\n",
    "                allele.state.sequence = str(sr[str(allele.location.sequence_id)][allele.location.start.value:allele.location.end.value])\n",
    "            allele = normalize(allele, data_proxy = dp)    \n",
    "            allele.id = ga4gh_identify(allele)\n",
    "            alleles.append(allele)\n",
    "        except:\n",
    "            vrstext = {'definition':ref + ':'+ l +'.' + varlist[i], 'type': 'Text'}\n",
    "            return vrstext\n",
    "    \n",
    "    if len(alleles) == 1: # Not haplotype\n",
    "        return alleles[0]\n",
    "    else:\n",
    "        return models.Haplotype(members = alleles)\n",
    "    \n",
    "def get_clingen_id(hgvs):\n",
    "    url = 'https://reg.genome.network/allele?hgvs=' + hgvs\n",
    "    page = requests.get(url).json()\n",
    "    page = page['@id']\n",
    "    try:\n",
    "        return page.split('/')[4]\n",
    "    except:\n",
    "        return 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "24d27355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['urn:mavedb:00000097-q-1', 'no transcripts found']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallybg/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/Bio/Seq.py:2880: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'urn:mavedb:00000041-a-1': ['NP_938033.1',\n",
       "  269,\n",
       "  'urn:mavedb:00000041-a-1',\n",
       "  True,\n",
       "  'NM_198291.3',\n",
       "  'MANE Select'],\n",
       " 'urn:mavedb:00000048-a-1': ['NP_003458.1',\n",
       "  1,\n",
       "  'urn:mavedb:00000048-a-1',\n",
       "  True,\n",
       "  'NM_003467.3',\n",
       "  'MANE Select'],\n",
       " 'urn:mavedb:00000107-a-1': ['NP_478059.1',\n",
       "  0,\n",
       "  'urn:mavedb:00000107-a-1',\n",
       "  True,\n",
       "  'NM_058179.4',\n",
       "  'MANE Select'],\n",
       " 'urn:mavedb:00000103-d-1': ['NP_002736.3',\n",
       "  0,\n",
       "  'urn:mavedb:00000103-d-1',\n",
       "  True,\n",
       "  'NM_002745.5',\n",
       "  'MANE Select'],\n",
       " 'urn:mavedb:00000061-b-1': ['NP_002871.1',\n",
       "  51,\n",
       "  'urn:mavedb:00000061-b-1',\n",
       "  True,\n",
       "  'NM_002880.4',\n",
       "  'MANE Select'],\n",
       " 'urn:mavedb:00000097-q-1': [],\n",
       " 'urn:mavedb:00000003-a-1': ['NP_009225.1',\n",
       "  1,\n",
       "  'urn:mavedb:00000003-a-1',\n",
       "  False,\n",
       "  'NM_007294.4',\n",
       "  'MANE Select'],\n",
       " 'urn:mavedb:00000097-i-1': ['NP_009225.1',\n",
       "  1630,\n",
       "  'urn:mavedb:00000097-i-1',\n",
       "  False,\n",
       "  'NM_007294.4',\n",
       "  'MANE Select'],\n",
       " 'urn:mavedb:00000099-a-1': ['NP_000530.1',\n",
       "  0,\n",
       "  'urn:mavedb:00000099-a-1',\n",
       "  True,\n",
       "  'NM_000539.3',\n",
       "  'MANE Select']}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## UTA Transcript Selection\n",
    "nest_asyncio.apply()\n",
    "mane = MANETranscriptMappings()\n",
    "utadb = UTADatabase('postgresql://anonymous:anonymous@uta.biocommons.org:5432/uta')\n",
    "qh = QueryHandler(create_db())\n",
    "dp = SeqRepoDataProxy(sr = sr)\n",
    "\n",
    "mappings_dict = {}\n",
    "mave_dat = pd.read_csv('mave_dat.csv')\n",
    "dat = mave_dat\n",
    "with open('mave_blat.pickle', 'rb') as fn:\n",
    "    mave_blat_dict = pickle.load(fn)\n",
    "\n",
    "for j in range(len(dat.index)):\n",
    "    if dat.at[j, 'target_type'] == 'Protein coding' or dat.at[j, 'target_type'] == 'protein_coding':\n",
    "        item = mave_blat_dict[dat.at[j,'urn']]\n",
    "        #if blat_check(j) == False:\n",
    "         #   mappings_dict[dat.at[j, 'urn']] = 'BLAT hit not found on correct chromosome'\n",
    "          #  continue\n",
    "        if item['chrom'] == 'NA':\n",
    "            continue\n",
    "        locs = get_locs_list(item['hits'])\n",
    "        chrom = get_chr(dp, item['chrom'])\n",
    "\n",
    "        uniprot = dat.at[j, 'uniprot_id']\n",
    "        uniprot_gene = qh.normalize(str(f'uniprot:{uniprot}')).gene\n",
    "        # try to normalize based on the uniprot_id, but if there is no match, normalize based on the first part of the target name\n",
    "        if uniprot_gene == None:\n",
    "            temp = dat.at[j, 'target'].split(' ')\n",
    "            if temp[0] == 'JAK':\n",
    "                temp[0] = 'JAK1'\n",
    "            gsymb = qh.normalize(temp[0]).gene.label\n",
    "        else:\n",
    "            gsymb = uniprot_gene.label\n",
    "\n",
    "\n",
    "        async def mapq():\n",
    "            transcript_lists = []\n",
    "            for i in range(len(locs)):\n",
    "                testquery = (f\"\"\"select *\n",
    "                            from uta_20210129.tx_exon_aln_v\n",
    "                            where hgnc = '{gsymb}'\n",
    "                            and {locs[i][0]} between alt_start_i and alt_end_i\n",
    "                            or {locs[i][1]} between alt_start_i and alt_end_i\n",
    "                            and alt_ac = '{chrom}'\"\"\") \n",
    "    \n",
    "                out = await utadb.execute_query(testquery)\n",
    "                tl = []\n",
    "                for j in range(len(out)):\n",
    "                    if out[j]['tx_ac'].startswith('NR_') == False:\n",
    "                        tl.append(out[j]['tx_ac'])\n",
    "                if tl != []:\n",
    "                    transcript_lists.append(tl)\n",
    "            return(transcript_lists)\n",
    "\n",
    "        ts = asyncio.run(mapq())\n",
    "        try:\n",
    "            isect = list(set.intersection(*map(set,ts)))\n",
    "        except:\n",
    "            try: # Look for transcripts using uniprot id\n",
    "                url = 'https://www.uniprot.org/uniprot/' + str(dat.at[j, 'uniprot_id']) + '.xml'\n",
    "                page = requests.get(url)\n",
    "                page = BeautifulSoup(page.text)\n",
    "                page = page.find_all('sequence')\n",
    "                up = page[1].get_text()\n",
    "\n",
    "                stri = str(dat.at[j,'target_sequence'])\n",
    "                if up.find(stri) != -1:\n",
    "                    full_match = True\n",
    "                else:\n",
    "                    full_match = False\n",
    "                start = up.find(stri[:10])\n",
    "                mappings_dict[dat.at[j,'urn']] = [dat.at[j, 'uniprot_id'], start, dat.at[j, 'urn'], full_match]\n",
    "                continue\n",
    "            except:\n",
    "                print([dat.at[j, 'urn'], 'no transcripts found'])\n",
    "                mappings_dict[dat.at[j,'urn']] = []\n",
    "                continue\n",
    "\n",
    "        mane_trans = mane.get_mane_from_transcripts(isect)\n",
    "        if mane_trans != []:\n",
    "            if len(mane_trans) == 1:\n",
    "                np = mane_trans[0]['RefSeq_prot']\n",
    "                nm = mane_trans[0]['RefSeq_nuc']\n",
    "                status = 'MANE Select'\n",
    "            else:\n",
    "                if mane_trans[0]['MANE_status'] == 'MANE Select':\n",
    "                    np = mane_trans[0]['RefSeq_prot']\n",
    "                    nm = mane_trans[0]['RefSeq_nuc']\n",
    "                    status = 'MANE Select'\n",
    "                else:\n",
    "                    np = mane_trans[1]['RefSeq_prot']\n",
    "                    nm = mane_trans[1]['RefSeq_nuc']\n",
    "                    status = 'MANE Plus Clinical'\n",
    "            \n",
    "            oseq = dat.at[j, 'target_sequence']\n",
    "            \n",
    "            if len(set(str(oseq))) > 4:\n",
    "                stri = str(oseq)\n",
    "            else:\n",
    "                oseq = Seq(oseq)\n",
    "                stri = str(oseq.translate(table=1)).replace('*', '')\n",
    "            \n",
    "            if str(sr[np]).find(stri) != -1:\n",
    "                full_match = True\n",
    "            else:\n",
    "                full_match = False\n",
    "            start = str(sr[np]).find(stri[:10])\n",
    "            mappings_dict[dat.at[j,'urn']] = [np, start, dat.at[j, 'urn'], full_match, nm, status]\n",
    "            \n",
    "        else:\n",
    "            trans_lens = []\n",
    "            for i in range(len(isect)):\n",
    "                trans_lens.append(len(str(sr[isect[i]])))\n",
    "            loc = trans_lens.index(max(trans_lens))\n",
    "            nm = isect[loc]\n",
    "    \n",
    "            testquery = f\"SELECT pro_ac FROM uta_20210129.associated_accessions WHERE tx_ac = '{nm}'\"\n",
    "            async def np():\n",
    "                out = await utadb.execute_query(testquery)\n",
    "                try:\n",
    "                    return out[0]['pro_ac']\n",
    "                except:\n",
    "                    return out\n",
    "            np = asyncio.run(np())\n",
    "            \n",
    "            if np != []:\n",
    "                oseq = dat.at[j, 'target_sequence']\n",
    "            \n",
    "                if len(set(str(oseq))) > 4:\n",
    "                    stri = str(oseq)\n",
    "                else:\n",
    "                    oseq = Seq(oseq)\n",
    "                    stri = str(oseq.translate(table=1)).replace('*', '')\n",
    "                \n",
    "                if str(sr[np]).find(stri) != -1:\n",
    "                    full_match = True\n",
    "                else:\n",
    "                    full_match = False\n",
    "                start = str(sr[np]).find(stri[:10])\n",
    "                mappings_dict[dat.at[j,'urn']] = [np, start, dat.at[j, 'urn'], full_match, nm, 'Longest Compatible'] \n",
    "mappings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9a3cb7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallybg/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/Bio/Seq.py:2880: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n",
      "  warnings.warn(\n",
      "/Users/sallybg/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/Bio/Seq.py:2880: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Find start location in provided target sequence when start position is not first position of sequence \n",
    "import operator\n",
    "offset_within_ts = {}\n",
    "\n",
    "def validation_helper(protstring):\n",
    "    protstring = protstring[1:][:-1]\n",
    "    vs = protstring.split(';')\n",
    "    return vs\n",
    "\n",
    "for i in range(len(mave_dat.index)):\n",
    "    if mave_dat.at[i, 'target_type'] == 'Protein coding' and mave_dat.at[i, 'target_sequence_type'] == 'dna':\n",
    "        urn = mave_dat.at[i, 'urn']\n",
    "        if urn == 'urn:mavedb:00000053-a-1' or urn == 'urn:mavedb:00000053-a-2': # target sequence missing codon\n",
    "            continue\n",
    "        oseq = Seq(mave_dat.at[i, 'target_sequence'])\n",
    "        ts = str(oseq.translate(table = 1))\n",
    "\n",
    "        string = 'https://api.mavedb.org/api/v1/score-sets/' + mave_dat.at[i, 'urn']+ '/scores'\n",
    "        origdat = requests.get(string).content\n",
    "        dat = pd.read_csv(io.StringIO(origdat.decode('utf-8')))\n",
    "\n",
    "        protlist = dat['hgvs_pro'].to_list()\n",
    "        if type(dat.at[0, 'hgvs_pro']) != str or dat.at[0, 'hgvs_pro'].startswith('NP'):\n",
    "            continue\n",
    "        protlist = [x.lstrip('p.') for x in protlist]\n",
    "    \n",
    "        aa_dict = {}\n",
    "        for k in range(len(protlist)):\n",
    "            if protlist[k] == '_sy' or protlist[k] == '_wt':\n",
    "                continue\n",
    "            else:\n",
    "                if ';' in protlist[k]: \n",
    "                    vs = validation_helper(protlist[k])\n",
    "                    for l in range(len(vs)):\n",
    "                        aa = vs[l][:3]\n",
    "                        if aa == '=' or vs[l][-3:] not in Bio.SeqUtils.IUPACData.protein_letters_3to1.keys():\n",
    "                            continue\n",
    "                        if '=' in vs[l]:\n",
    "                            loc = vs[l][3:][:-1]\n",
    "                        else:\n",
    "                            loc = vs[l][3:][:-3]\n",
    "                        if loc not in aa_dict:\n",
    "                            loc = re.sub('[^0-9]', '', loc)\n",
    "                            aa_dict[loc] = seq1(aa)\n",
    "                                \n",
    "                else:\n",
    "                    if '_' in protlist[k]:\n",
    "                        continue\n",
    "                    aa = protlist[k][:3]\n",
    "                    if aa == '=' or protlist[k][-3:] not in Bio.SeqUtils.IUPACData.protein_letters_3to1.keys():\n",
    "                        continue\n",
    "                    if '=' in protlist[k]:\n",
    "                        loc = protlist[k][3:][:-1]\n",
    "                    else:\n",
    "                        loc = protlist[k][3:][:-3]\n",
    "                    if loc not in aa_dict:\n",
    "                        loc = re.sub('[^0-9]', '', loc)\n",
    "                        aa_dict[loc] = seq1(aa)\n",
    "                            \n",
    "\n",
    "        aa_dict.pop('', None)\n",
    "        \n",
    "        err_locs = []\n",
    "        for m in range(len(ts)):\n",
    "            if str(m) in list(aa_dict.keys()):\n",
    "                if aa_dict[str(m)] != ts[int(m) - 1]: # Str vs dict offset\n",
    "                    err_locs.append(m)\n",
    "        \n",
    "        if len(err_locs) > 1:\n",
    "            aa_dict = {int(k):v for k,v in aa_dict.items()}\n",
    "            aa_dict = sorted(aa_dict.items())\n",
    "            aa_dict = dict(aa_dict)\n",
    "            locs = list(aa_dict.keys())[0:5]\n",
    "            p0, p1, p2, p3, p4 = locs[0], locs[1], locs[2], locs[3], locs[4]\n",
    "            offset = locs[0]\n",
    "\n",
    "            seq = ''\n",
    "            for key in aa_dict:\n",
    "                seq = seq + aa_dict[key]\n",
    "                \n",
    "            for i in range(len(ts)):\n",
    "                if ts[i] == aa_dict[p0] and ts[i + p1 - p0] == aa_dict[p1] and ts[i + p2 - p0] == aa_dict[p2] and ts[i + p3 - p0] == aa_dict[p3] and ts[i + p4 - p0] == aa_dict[p4]:\n",
    "                    if i + 1 == min(aa_dict.keys()) or i + 2 == min(aa_dict.keys()):\n",
    "                        offset_within_ts[urn] = 0\n",
    "                    else:\n",
    "                        offset_within_ts[urn] = i\n",
    "                    break\n",
    "\n",
    "for key in offset_within_ts:\n",
    "    mappings_dict[key][1] = offset_within_ts[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cfdad34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'urn:mavedb:00000041-a-1': ['NP_938033.1',\n",
       "  269,\n",
       "  'urn:mavedb:00000041-a-1',\n",
       "  True,\n",
       "  'NM_198291.3',\n",
       "  'MANE Select'],\n",
       " 'urn:mavedb:00000048-a-1': ['NP_003458.1',\n",
       "  0,\n",
       "  'urn:mavedb:00000048-a-1',\n",
       "  True,\n",
       "  'NM_003467.3',\n",
       "  'MANE Select'],\n",
       " 'urn:mavedb:00000107-a-1': ['NP_478059.1',\n",
       "  0,\n",
       "  'urn:mavedb:00000107-a-1',\n",
       "  True,\n",
       "  'NM_058179.4',\n",
       "  'MANE Select'],\n",
       " 'urn:mavedb:00000103-d-1': ['NP_002736.3',\n",
       "  0,\n",
       "  'urn:mavedb:00000103-d-1',\n",
       "  True,\n",
       "  'NM_002745.5',\n",
       "  'MANE Select'],\n",
       " 'urn:mavedb:00000061-b-1': ['NP_002871.1',\n",
       "  51,\n",
       "  'urn:mavedb:00000061-b-1',\n",
       "  True,\n",
       "  'NM_002880.4',\n",
       "  'MANE Select'],\n",
       " 'urn:mavedb:00000097-q-1': [],\n",
       " 'urn:mavedb:00000003-a-1': ['NP_009225.1',\n",
       "  1,\n",
       "  'urn:mavedb:00000003-a-1',\n",
       "  False,\n",
       "  'NM_007294.4',\n",
       "  'MANE Select'],\n",
       " 'urn:mavedb:00000097-i-1': ['NP_009225.1',\n",
       "  1630,\n",
       "  'urn:mavedb:00000097-i-1',\n",
       "  False,\n",
       "  'NM_007294.4',\n",
       "  'MANE Select'],\n",
       " 'urn:mavedb:00000099-a-1': ['NP_000530.1',\n",
       "  0,\n",
       "  'urn:mavedb:00000099-a-1',\n",
       "  True,\n",
       "  'NM_000539.3',\n",
       "  'MANE Select']}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "37a7890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mappings.pickle', 'wb') as fn:\n",
    "    pickle.dump(mappings_dict, fn, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c59aa0",
   "metadata": {},
   "source": [
    "## Part 3: Transcript to VRS Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86c4681",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mave_blat.pickle', 'rb') as fn:\n",
    "    mave_blat_dict = pickle.load(fn)\n",
    "    \n",
    "with open('mappings.pickle', 'rb') as fn:\n",
    "    mappings_dict = pickle.load(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510b9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_haplotype_allele_mavehgvs(var, ref, offset, l, tr, dp, ts, mapped, ranges, hits, strand):\n",
    "    var = var.lstrip(f'{l}.')\n",
    "    if '[' in var:\n",
    "        var = var[1:][:-1]\n",
    "        varlist = var.split(';')\n",
    "        varlist = list(set(varlist))\n",
    "    else:\n",
    "        varlist = list()\n",
    "        varlist.append(var)\n",
    "\n",
    "    locs = {}\n",
    "    alleles = []\n",
    "\n",
    "    for i in range(len(varlist)):\n",
    "        # hgvs_string = ref + ':'+ l +'.' + varlist[i]\n",
    "        # allele = tr.translate_from(hgvs_string, 'hgvs')\n",
    "        \n",
    "        if mapped == 'pre':\n",
    "            hgvs_string = ref + ':'+ l +'.' + varlist[i]\n",
    "\n",
    "            # TODO protein fs hgvs strings are not supported because they can't be handled by vrs allele translator\n",
    "            if re.search(mavehgvs.patterns.protein.pro_fs, hgvs_string):\n",
    "                raise NotImplementedError(\"Pre-map VRS translation not supported for fs variants denoted with protein hgvs strings\")\n",
    "            \n",
    "            # TODO multi position variants\n",
    "            # this actually works for pre-map, but don't support it until post-map works\n",
    "            if re.search(mavehgvs.patterns.protein.pro_multi_variant, hgvs_string):\n",
    "                raise NotImplementedError(\"Pre-map VRS translation not supported for multi-position variants\")\n",
    "\n",
    "            allele = tr.translate_from(hgvs_string, 'hgvs')\n",
    "            # it's necessary to update the sequence identifier after translation, rather than including it in the hgvs string,\n",
    "            # because the hgvs parser expects a digit after the 'SQ.'\n",
    "            # note: not updating sequence reference until after normalization,\n",
    "            # because computed sequence identifier should include 'ga4gh:SQ', (see example here https://vrs.ga4gh.org/en/1.1/impl-guide/example.html)\n",
    "            # and the 'ga4gh:' breaks the normalizer\n",
    "            #allele.location.sequenceReference.refgetAccession = 'SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "\n",
    "            if 'dup' in hgvs_string:\n",
    "                print('dup in hgvs string. this has not been tested yet, review output.')\n",
    "                allele.state.sequence.root = 2*str(sr[str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "                \n",
    "        else:\n",
    "            if l != 'g':\n",
    "                # TODO do we need to do anything for negative strand if using p. hgvs?\n",
    "                # expecting protein-based ref, so hgvs string is already mostly correct - just need to calculate offset\n",
    "                # could parse whole list outside of for loop since this function takes a list\n",
    "                parsed_hgvs = mavehgvs.util.parse_variant_strings(['p.' + varlist[i]])[0][0]\n",
    "                # looks like offset is calculated based on amino acids, so this should be correct, but should validate\n",
    "                # may want to only do this if offset != 0? i guess that depends on how often offset == 0\n",
    "\n",
    "                # TODO positions can be a tuple if there are multiple positions associated with the variant.\n",
    "                # if positions is a tuple, accessing position like this won't work.\n",
    "                # so need to check length of parsed_hgvs.positions\n",
    "                # should we expect multi-position protein variants?\n",
    "                # looks like yes - example from mavehgvs spec: p.His7_Gln8insSer\n",
    "                \n",
    "                if not isinstance(parsed_hgvs.positions, mavehgvs.position.VariantPosition):\n",
    "                    raise NotImplementedError(\"Post-map VRS translation for protein-coding variants spanning multiple positions has not been implemented.\")\n",
    "                \n",
    "                # TODO protein fs hgvs strings are not supported because they can't be handled by vrs allele translator\n",
    "                if re.search(mavehgvs.patterns.protein.pro_fs, str(parsed_hgvs)):\n",
    "                    raise NotImplementedError(\"Post-map VRS translation not supported for fs variants denoted with protein hgvs strings\")\n",
    "\n",
    "                parsed_hgvs.positions.position = parsed_hgvs.positions.position + offset\n",
    "                hgvs_string = ref + ':' + str(parsed_hgvs)\n",
    "                allele = tr.translate_from(hgvs_string, 'hgvs')\n",
    "\n",
    "                # allele.location.start = allele.location.start + offset\n",
    "                # allele.location.end = allele.location.end + offset\n",
    "                # dups haven't been fixed yet, need to find a test case\n",
    "                if 'dup' in hgvs_string:\n",
    "                    # not sure if this needs to be allele.state.sequence.root\n",
    "                    print('dup in hgvs string. this has not been tested yet, review output.')\n",
    "                    allele.state.sequence.root = 2*str(sr[str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "                    \n",
    "            else:\n",
    "                # can we assume that the noncoding hgvs strings coming in from mavedb in the hgvs_nt column are c.?\n",
    "                parsed_hgvs = mavehgvs.util.parse_variant_strings(['c.' + varlist[i]])[0][0]\n",
    "                # start = allele.location.start\n",
    "                if not isinstance(parsed_hgvs.positions, mavehgvs.position.VariantPosition):\n",
    "                    raise NotImplementedError(\"Post-map VRS translation for non-protein-coding variants spanning multiple positions has not been implemented.\")\n",
    "                \n",
    "                start = parsed_hgvs.positions.position - 1 #hgvs uses 1-based numbering for c. sequences, while blat hits are 0-based\n",
    "\n",
    "                # get hit\n",
    "                if len(hits) == 1:\n",
    "                    i = 0\n",
    "                else:\n",
    "                    for i in range(len(hits)):\n",
    "                        if start >= hits[i][0] and start < hits[i][1]:\n",
    "                            break\n",
    "\n",
    "                # if hit is on positive strand\n",
    "                if strand == 1:\n",
    "                    # get variant start relative to the reference (the \"hit\")\n",
    "                    # distance from beginning of query to variant start position:\n",
    "                    query_to_start = start - hits[i][0]\n",
    "                    # distance from beginning of ref to the variant start position:\n",
    "                    ref_to_start = ranges[i][0] + query_to_start\n",
    "                    # hgvs is 1-based, so convert back to 1-based\n",
    "                    parsed_hgvs.positions.position = ref_to_start + 1\n",
    "                # if hit is on negative strand    \n",
    "                else:\n",
    "                    # in this case, picture the rev comp of the query/variant as mapping to the positive strand of the ref\n",
    "                    # the start of the reverse complement of the variant is the end of the \"original\" variant\n",
    "                    # so we need to know where the end of the original variant is, relative to the query molecule\n",
    "                    # for single-position variants, we'll assume the end (rev comp view) is equal to: start - 1       \n",
    "                    # TODO this works for single-position variants only!\n",
    "                    # this error is redundant (should be caught above),\n",
    "                    # but since it's not necessarily obvious that this works for\n",
    "                    # single-position variants only,\n",
    "                    # I'm putting it here as well because development\n",
    "                    # will need to happen here as well in order to support multi-position\n",
    "                    # variants, since diff2 = 1 is ONLY a good assumption for single-position variants\n",
    "                    if not isinstance(parsed_hgvs.positions, mavehgvs.position.VariantPosition):\n",
    "                        raise NotImplementedError(\"Post-map VRS translation for protein-coding variants spanning multiple positions has not been implemented.\")\n",
    "                    \n",
    "                    # the distance between the start and end of the variant is dependent on the number of positions covered by the variant!\n",
    "                    # this is hardcoded for single-position variants, for now\n",
    "                    end = start\n",
    "                    # subtract 1 from end of hit range, because blat ranges are 0-based [start, end)\n",
    "                    ref_to_start = (ranges[i][1] -1 ) - (end - hits[i][0])\n",
    "                    # or could do ranges[i][0] + (end - hits[i][1]), is one better than the other? any cases where one might be inaccurate?\n",
    "                    # hgvs is 1-based, so convert back to 1-based\n",
    "                    parsed_hgvs.positions.position = ref_to_start + 1\n",
    "\n",
    "                    # rev comp each sequence, assuming [0] is original and [1] is variant\n",
    "                    # this is only tested for single position variants\n",
    "\n",
    "                    revcomp_sequences_list = []\n",
    "                    for sequence in parsed_hgvs._sequences:\n",
    "                        revcomp_sequences_list.append(str(Seq(sequence).reverse_complement()))\n",
    "                    parsed_hgvs._sequences = tuple(revcomp_sequences_list)\n",
    "\n",
    "                # get hgvs and allele\n",
    "                hgvs_string = ref + ':' + str(parsed_hgvs)\n",
    "                allele = tr.translate_from(hgvs_string, 'hgvs')\n",
    "        \n",
    "        # TODO dups will need to be corrected after the allele object is created, because the mavehgvs string\n",
    "        # does not contain information about the identity of the base that is duplicated\n",
    "        # not immediately sure how to handle rev comp dups\n",
    "\n",
    "        # haven't fixed this if block yet, need test case\n",
    "        # not sure if this needs to be allele.state.sequence.root\n",
    "        if allele.state.sequence.root == 'N' and l != 'p':\n",
    "            print('sequence is N. this has not been tested yet, review output.')\n",
    "            allele.state.sequence.root = str(sr[str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "        allele = normalize(allele, data_proxy = dp)\n",
    "    \n",
    "        # update sequence reference id after normalization, see commented notes in pre mapping section above\n",
    "        if mapped == 'pre':\n",
    "            # not sure if refgetAccession is the appropriate field to update here, since this is a ga4gh computed seq id.\n",
    "            # do ga4gh computed seq ids count as refget accession ids?\n",
    "            allele.location.sequenceReference.refgetAccession = 'ga4gh:SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "        allele.id = ga4gh_identify(allele)\n",
    "        alleles.append(allele)\n",
    "    \n",
    "    if len(alleles) == 1: # Not haplotype\n",
    "        return alleles[0]\n",
    "    else:\n",
    "        return models.Haplotype(members = alleles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9efe716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urn:mavedb:00000099-a-1\n",
      "skipping, pre-map fs\n",
      "p.Glu341fs\n",
      "skipping, pre-map fs\n",
      "p.Phe13fs\n",
      "skipping, post-map multi position variant\n",
      "p.Val137_Pro142del\n",
      "skipping, pre-map fs\n",
      "p.Leu328fs\n",
      "skipping, pre-map fs\n",
      "p.Asn315fs\n",
      "skipping, pre-map fs\n",
      "p.Ser334fs\n",
      "skipping, pre-map fs\n",
      "p.Ala335fs\n",
      "skipping, post-map multi position variant\n",
      "p.Tyr206_Phe208del\n",
      "skipping, pre-map fs\n",
      "p.Thr340fs\n",
      "skipping, pre-map fs\n",
      "p.Glu341fs\n",
      "skipping, pre-map fs\n",
      "p.Glu332fs\n",
      "skipping, pre-map fs\n",
      "p.Pro327fs\n",
      "skipping, post-map multi position variant\n",
      "p.Arg69_Leu72del\n",
      "skipping, pre-map fs\n",
      "p.Pro327fs\n",
      "skipping, post-map multi position variant\n",
      "p.Leu318_Thr319delinsPro\n",
      "skipping, pre-map fs\n",
      "p.Ter349fs\n",
      "skipping, pre-map fs\n",
      "p.Ter349fs\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb Cell 43\u001b[0m line \u001b[0;36m8\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X35sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m             \u001b[39mprint\u001b[39m(varm[j])\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X35sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X35sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m tempdat \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m'\u001b[39;49m\u001b[39mpre_mapping\u001b[39;49m\u001b[39m'\u001b[39;49m: var_ids_pre_map, \u001b[39m'\u001b[39;49m\u001b[39mmapped\u001b[39;49m\u001b[39m'\u001b[39;49m: var_ids_post_map})\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X35sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m mappings_list\u001b[39m.\u001b[39mappend(tempdat)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X35sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m scores_list\u001b[39m.\u001b[39mappend(spro)\n",
      "\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n",
      "\u001b[1;32m    761\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n",
      "\u001b[1;32m    762\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n",
      "\u001b[1;32m    763\u001b[0m     )\n",
      "\u001b[1;32m    765\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n",
      "\u001b[1;32m    766\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n",
      "\u001b[0;32m--> 767\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n",
      "\u001b[1;32m    768\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n",
      "\u001b[1;32m    769\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n",
      "\u001b[1;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m    500\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n",
      "\u001b[1;32m    501\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n",
      "\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n",
      "\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n",
      "\u001b[1;32m    112\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n",
      "\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 114\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n",
      "\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m    116\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n",
      "\u001b[1;32m    675\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n",
      "\u001b[1;32m    676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;32m--> 677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m    679\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n",
      "\u001b[1;32m    680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n",
      "\u001b[1;32m    681\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    682\u001b[0m     )\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# VRS Variant Mapping - Coding Scoresets\n",
    "dp = SeqRepoDataProxy(sr = sr)\n",
    "tr = AlleleTranslator(data_proxy = dp, normalize = False)\n",
    "qh = QueryHandler(create_db())\n",
    "vrs_mappings_dict = {}\n",
    "scores_dict_coding = {}\n",
    "mavedb_ids_coding = {}\n",
    "\n",
    "mave_dat = pd.read_csv('mave_dat.csv')\n",
    "dat = mave_dat\n",
    "\n",
    "# for each urn in the mave data requested from mavedb:\n",
    "for i in range(len(dat.index)):\n",
    "    # this section only processes protein coding sequences\n",
    "    if dat.at[i, 'target_type'] == 'Protein coding' or dat.at[i, 'target_type'] == 'protein_coding':\n",
    "        # if there is a mapping entry for this urn:\n",
    "        if dat.at[i, 'urn'] in mappings_dict.keys():\n",
    "            print(dat.at[i, 'urn'])\n",
    "            # grab the urn's mapping entry\n",
    "            item = mappings_dict[dat.at[i, 'urn']]\n",
    "            # get scoreset for this urn from mavedb\n",
    "            string = 'https://api.mavedb.org/api/v1/score-sets/' + mave_dat.at[i, 'urn']+ '/scores'\n",
    "            origdat = requests.get(string).content\n",
    "            vardat = pd.read_csv(io.StringIO(origdat.decode('utf-8')))\n",
    "            scores = vardat['score'].to_list()\n",
    "            accessions = vardat['accession'].to_list()\n",
    "            \n",
    "            mappings_list = []\n",
    "            scores_list = []\n",
    "            accessions_list = []\n",
    "        \n",
    "            # Process protein column\n",
    "            var_ids_pre_map = []\n",
    "            var_ids_post_map = []\n",
    "            \n",
    "            if len(item) != 0:\n",
    "                np = item[0]\n",
    "                offset = item[1]\n",
    "            varm = vardat['hgvs_pro']\n",
    "        \n",
    "            ts = dat.at[i, 'target_sequence']\n",
    "            if len(set(str(ts))) > 4:\n",
    "                stri = str(ts)\n",
    "            \n",
    "            else:\n",
    "                ts = Seq(ts)\n",
    "                ts = str(ts.translate(table=1)).replace('*', '')\n",
    "                \n",
    "            digest = 'SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "            alias_dict_list = [{'namespace': 'ga4gh', 'alias': digest}]\n",
    "            sr.store(ts, nsaliases = alias_dict_list) # Add custom digest to SeqRepo\n",
    "            \n",
    "            spro = []\n",
    "            accpro = []\n",
    "            \n",
    "            for j in range(len(varm)):\n",
    "                if type(varm[j]) != str or len(varm[j]) == 3 or varm[j] == '_wt' or varm[j] == '_sy':\n",
    "                    continue\n",
    "                if varm[j].startswith('NP') == True:\n",
    "                    var_ids_pre_map.append(tr.translate_from(varm[j], 'hgvs'))\n",
    "                    var_ids_post_map.append(tr.translate_from(varm[j], 'hgvs'))\n",
    "                    spro.append(scores[j])\n",
    "                    accpro.append(accessions[j])\n",
    "                else:\n",
    "                    try:\n",
    "                        if np.startswith('N') == True:\n",
    "                            var_ids_pre_map.append(get_haplotype_allele_mavehgvs(varm[j], np, 0, 'p', tr, dp, ts, 'pre', '', '', ''))\n",
    "                            var_ids_post_map.append(get_haplotype_allele_mavehgvs(varm[j], np, offset, 'p', tr, dp, ts, 'post', '', '', ''))\n",
    "                            spro.append(scores[j])\n",
    "                            accpro.append(accessions[j])\n",
    "                        else:\n",
    "                            var_ids_pre_map.append(get_haplotype_allele_mavehgvs(varm[j], np, 0, 'p', tr, dp, ts, 'pre', '', '', ''))\n",
    "                            # TODO ranges and hits don't actually get used by get_haplotype_allele, are they intended to be used here?\n",
    "                            # what is the 'np' that we're expecting here if it doesn't start with 'N'?\n",
    "                            var_ids_post_map.append(get_haplotype_allele_mavehgvs(varm[j], np, offset, 'p', tr, dp, ts, 'post', ranges, hits, ''))\n",
    "                            spro.append(scores[j])\n",
    "                            accpro.append(accessions[j])\n",
    "                    except:\n",
    "                        continue\n",
    "                    \n",
    "            tempdat = pd.DataFrame({'pre_mapping': var_ids_pre_map, 'mapped': var_ids_post_map})\n",
    "            mappings_list.append(tempdat)\n",
    "            scores_list.append(spro)\n",
    "            accessions_list.append(accpro)\n",
    "            \n",
    "            # Process nt column if data present\n",
    "            if vardat['hgvs_nt'].isnull().values.all() == False and '97' not in dat.at[i, 'urn']:\n",
    "                var_ids_pre_map = []\n",
    "                var_ids_post_map = []\n",
    "            \n",
    "                item = mave_blat_dict[dat.at[i, 'urn']]\n",
    "                ranges = get_locs_list(item['hits'])\n",
    "                hits = get_hits_list(item['hits'])\n",
    "                ref = get_chr(dp, item['chrom'])\n",
    "                ts = dat.at[i, 'target_sequence']\n",
    "                strand = mave_blat_dict[dat.at[i, 'urn']]['strand']\n",
    "                \n",
    "                digest = 'SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "                alias_dict_list = [{'namespace': 'ga4gh', 'alias': digest}]\n",
    "                sr.store(ts, nsaliases = alias_dict_list) # Add custom digest to SeqRepo\n",
    "            \n",
    "                ntlist = vardat['hgvs_nt']\n",
    "                varm = vardat['hgvs_pro']\n",
    "                sn = []\n",
    "                accn = []\n",
    "            \n",
    "                for j in range(len(ntlist)):\n",
    "                    if type(ntlist[j]) != str or ntlist[j] == '_wt' or ntlist[j] == '_sy':\n",
    "                        continue\n",
    "                    else:\n",
    "                        try:\n",
    "                            var_ids_pre_map.append(get_haplotype_allele_mavehgvs(ntlist[j][2:], ref, 0, 'g', tr, dp, ts,'pre', ranges, hits, strand).as_dict())\n",
    "                            var_ids_post_map.append(get_haplotype_allele_mavehgvs(ntlist[j][2:], ref, 0, 'g', tr, dp, ts,'post', ranges, hits, strand).as_dict())\n",
    "                            sn.append(scores[j])\n",
    "                            accn.append(accessions[j])\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                tempdat = pd.DataFrame({'pre_mapping': var_ids_pre_map, 'mapped': var_ids_post_map})\n",
    "                mappings_list.append(tempdat)\n",
    "                scores_list.append(sn)\n",
    "                accessions_list.append(accn)\n",
    "            \n",
    "        vrs_mappings_dict[dat.at[i, 'urn']] = mappings_list\n",
    "        scores_dict_coding[dat.at[i, 'urn']] = scores_list\n",
    "        mavedb_ids_coding[dat.at[i, 'urn']] = accessions_list\n",
    "vrs_mappings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70258f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urn:mavedb:00000018-a-1\n",
      "index: 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb Cell 25\u001b[0m line \u001b[0;36m5\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X36sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m         \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X36sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X36sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m tempdat \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m'\u001b[39;49m\u001b[39mpre_mapping\u001b[39;49m\u001b[39m'\u001b[39;49m: var_ids_pre_map, \u001b[39m'\u001b[39;49m\u001b[39mmapped\u001b[39;49m\u001b[39m'\u001b[39;49m: var_ids_post_map})\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X36sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m vrs_noncoding_mappings_dict[dat\u001b[39m.\u001b[39mat[i, \u001b[39m'\u001b[39m\u001b[39murn\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m tempdat\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X36sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m scores_dict_noncoding[dat\u001b[39m.\u001b[39mat[i, \u001b[39m'\u001b[39m\u001b[39murn\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m scores_list\n",
      "\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n",
      "\u001b[1;32m    761\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n",
      "\u001b[1;32m    762\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n",
      "\u001b[1;32m    763\u001b[0m     )\n",
      "\u001b[1;32m    765\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n",
      "\u001b[1;32m    766\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n",
      "\u001b[0;32m--> 767\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n",
      "\u001b[1;32m    768\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n",
      "\u001b[1;32m    769\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n",
      "\u001b[1;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m    500\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n",
      "\u001b[1;32m    501\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n",
      "\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n",
      "\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n",
      "\u001b[1;32m    112\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n",
      "\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 114\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n",
      "\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m    116\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n",
      "\u001b[1;32m    675\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n",
      "\u001b[1;32m    676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;32m--> 677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m    679\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n",
      "\u001b[1;32m    680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n",
      "\u001b[1;32m    681\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    682\u001b[0m     )\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# VRS variant mapping non-protein coding scoresets\n",
    "dp = SeqRepoDataProxy(sr = sr)\n",
    "tr = AlleleTranslator(data_proxy = dp, normalize = False)\n",
    "qh = QueryHandler(create_db())\n",
    "vrs_noncoding_mappings_dict = {}\n",
    "scores_dict_noncoding = {}\n",
    "mavedb_ids_noncoding = {}\n",
    "\n",
    "mave_dat = pd.read_csv('mave_dat.csv')\n",
    "dat = mave_dat\n",
    "\n",
    "for i in range(len(dat.index)):\n",
    "    if dat.at[i, 'target_type'] != 'Protein coding' and dat.at[i, 'target_type'] != 'protein_coding':\n",
    "        print(dat.at[i, 'urn'])\n",
    "        item = mave_blat_dict[dat.at[i, 'urn']]\n",
    "        #if blat_check(i) == False:\n",
    "         #   vrs_noncoding_mappings_dict[dat.at[i, 'urn']] = 'BLAT hit not found on correct chromosome'\n",
    "          #  continue\n",
    "        #ranges = get_locs_list(item['hits'])[0]\n",
    "        string = string = 'https://api.mavedb.org/api/v1/score-sets/' + mave_dat.at[i, 'urn']+ '/scores'\n",
    "        origdat = requests.get(string).content\n",
    "        varsdat = pd.read_csv(io.StringIO(origdat.decode('utf-8')))\n",
    "        ntlist = varsdat['hgvs_nt'].to_list()\n",
    "    \n",
    "        var_ids_pre_map = []\n",
    "        var_ids_post_map = []\n",
    "        ranges = get_locs_list(item['hits'])\n",
    "        ref = get_chr(dp, item['chrom'])\n",
    "        hits = get_hits_list(item['hits'])\n",
    "        strand = mave_blat_dict[dat.at[i, 'urn']]['strand']\n",
    "        \n",
    "        ts = dat.at[i, 'target_sequence']\n",
    "        digest = 'SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "        alias_dict_list = [{'namespace': 'ga4gh', 'alias': digest}]\n",
    "        sr.store(ts, nsaliases = alias_dict_list) # Add custom digest to SeqRepo\n",
    "        \n",
    "        scores = varsdat['score'].to_list()\n",
    "        scores_list = []\n",
    "        accessions = varsdat['accession'].to_list()\n",
    "        accessions_list = []\n",
    "\n",
    "        for j in range(len(ntlist)):\n",
    "            if ntlist[j] == '_wt' or ntlist[j] == '_sy':\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    var_ids_pre_map.append(get_haplotype_allele_temp(ntlist[j][2:], ref, 0, 'g', tr, dp, ts, 'pre', ranges, hits, strand))\n",
    "                    var_ids_post_map.append(get_haplotype_allele_temp(ntlist[j][2:], ref, 0, 'g', tr, dp, ts, 'post', ranges, hits, strand))\n",
    "                    scores_list.append(scores[j])\n",
    "                    accessions_list.append(accessions[j])\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        tempdat = pd.DataFrame({'pre_mapping': var_ids_pre_map, 'mapped': var_ids_post_map})\n",
    "        vrs_noncoding_mappings_dict[dat.at[i, 'urn']] = tempdat\n",
    "        scores_dict_noncoding[dat.at[i, 'urn']] = scores_list\n",
    "        mavedb_ids_noncoding[dat.at[i, 'urn']] = accessions_list\n",
    "\n",
    "vrs_noncoding_mappings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a1e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d05568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below this is stuff that I used for testing and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c2ed68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp - get single score set for protein coding gene, for testing\n",
    "\n",
    "dp = SeqRepoDataProxy(sr = sr)\n",
    "tr = AlleleTranslator(data_proxy = dp, normalize = False)\n",
    "qh = QueryHandler(create_db())\n",
    "vrs_mappings_dict = {}\n",
    "scores_dict_coding = {}\n",
    "mavedb_ids_coding = {}\n",
    "\n",
    "mave_dat = pd.read_csv('mave_dat.csv')\n",
    "dat = mave_dat\n",
    "\n",
    "item = mappings_dict[dat.at[0, 'urn']]\n",
    "\n",
    "string = 'https://api.mavedb.org/api/v1/score-sets/' + mave_dat.at[0, 'urn']+ '/scores'\n",
    "origdat = requests.get(string).content\n",
    "vardat = pd.read_csv(io.StringIO(origdat.decode('utf-8')))\n",
    "scores = vardat['score'].to_list()\n",
    "accessions = vardat['accession'].to_list()\n",
    "\n",
    "mappings_list = []\n",
    "scores_list = []\n",
    "accessions_list = []\n",
    "\n",
    "# Process protein column\n",
    "var_ids_pre_map = []\n",
    "var_ids_post_map = []\n",
    "\n",
    "if len(item) != 0:\n",
    "    np = item[0]\n",
    "    offset = item[1]\n",
    "varm = vardat['hgvs_pro']\n",
    "\n",
    "ts = dat.at[0, 'target_sequence']\n",
    "if len(set(str(ts))) > 4:\n",
    "    stri = str(ts)\n",
    "\n",
    "else:\n",
    "    ts = Seq(ts)\n",
    "    ts = str(ts.translate(table=1)).replace('*', '')\n",
    "    \n",
    "digest = 'SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "alias_dict_list = [{'namespace': 'ga4gh', 'alias': digest}]\n",
    "sr.store(ts, nsaliases = alias_dict_list) # Add custom digest to SeqRepo\n",
    "\n",
    "spro = []\n",
    "accpro = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3ba5943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urn:mavedb:00000018-a-1\n"
     ]
    }
   ],
   "source": [
    "# temp - get single scoreset for non-protein coding gene, for testing\n",
    "\n",
    "dp = SeqRepoDataProxy(sr = sr)\n",
    "tr = AlleleTranslator(data_proxy = dp, normalize = False)\n",
    "qh = QueryHandler(create_db())\n",
    "vrs_noncoding_mappings_dict = {}\n",
    "scores_dict_noncoding = {}\n",
    "mavedb_ids_noncoding = {}\n",
    "\n",
    "mave_dat = pd.read_csv('mave_dat.csv')\n",
    "dat = mave_dat\n",
    "\n",
    "print(dat.at[2, 'urn'])\n",
    "item = mave_blat_dict[dat.at[2, 'urn']]\n",
    "#if blat_check(i) == False:\n",
    "    #   vrs_noncoding_mappings_dict[dat.at[i, 'urn']] = 'BLAT hit not found on correct chromosome'\n",
    "    #  continue\n",
    "#ranges = get_locs_list(item['hits'])[0]\n",
    "string = string = 'https://api.mavedb.org/api/v1/score-sets/' + mave_dat.at[2, 'urn']+ '/scores'\n",
    "origdat = requests.get(string).content\n",
    "varsdat = pd.read_csv(io.StringIO(origdat.decode('utf-8')))\n",
    "ntlist = varsdat['hgvs_nt'].to_list()\n",
    "\n",
    "var_ids_pre_map = []\n",
    "var_ids_post_map = []\n",
    "ranges = get_locs_list(item['hits'])\n",
    "ref = get_chr(dp, item['chrom'])\n",
    "hits = get_hits_list(item['hits'])\n",
    "strand = mave_blat_dict[dat.at[2, 'urn']]['strand']\n",
    "\n",
    "ts = dat.at[2, 'target_sequence']\n",
    "digest = 'SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "alias_dict_list = [{'namespace': 'ga4gh', 'alias': digest}]\n",
    "sr.store(ts, nsaliases = alias_dict_list) # Add custom digest to SeqRepo\n",
    "\n",
    "scores = varsdat['score'].to_list()\n",
    "scores_list = []\n",
    "accessions = varsdat['accession'].to_list()\n",
    "accessions_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e5ecb85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rev comp score set for testing\n",
    "dp = SeqRepoDataProxy(sr = sr)\n",
    "tr = AlleleTranslator(data_proxy = dp, normalize = False)\n",
    "qh = QueryHandler(create_db())\n",
    "vrs_mappings_dict = {}\n",
    "scores_dict_coding = {}\n",
    "mavedb_ids_coding = {}\n",
    "\n",
    "i = 9\n",
    "\n",
    "item = mappings_dict[dat.at[i, 'urn']]\n",
    "#if blat_check(i) == False:\n",
    "    #   vrs_mappings_dict[dat.at[i, 'urn']] = 'BLAT hit not found on correct chromosome'\n",
    "    #  continue\n",
    "# get scoreset for this urn from mavedb\n",
    "string = 'https://api.mavedb.org/api/v1/score-sets/' + mave_dat.at[i, 'urn']+ '/scores'\n",
    "origdat = requests.get(string).content\n",
    "vardat = pd.read_csv(io.StringIO(origdat.decode('utf-8')))\n",
    "scores = vardat['score'].to_list()\n",
    "accessions = vardat['accession'].to_list()\n",
    "\n",
    "mappings_list = []\n",
    "scores_list = []\n",
    "accessions_list = []\n",
    "\n",
    "item = mave_blat_dict[dat.at[9, 'urn']]\n",
    "ranges = get_locs_list(item['hits'])\n",
    "hits = get_hits_list(item['hits'])\n",
    "ref = get_chr(dp, item['chrom'])\n",
    "ts = dat.at[9, 'target_sequence']\n",
    "strand = mave_blat_dict[dat.at[9, 'urn']]['strand']\n",
    "\n",
    "digest = 'SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "alias_dict_list = [{'namespace': 'ga4gh', 'alias': digest}]\n",
    "sr.store(ts, nsaliases = alias_dict_list) # Add custom digest to SeqRepo\n",
    "\n",
    "ntlist = vardat['hgvs_nt']\n",
    "varm = vardat['hgvs_pro']\n",
    "sn = []\n",
    "accn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3f25103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scoreset with frameshifts for testing\n",
    "dp = SeqRepoDataProxy(sr = sr)\n",
    "tr = AlleleTranslator(data_proxy = dp, normalize = False)\n",
    "qh = QueryHandler(create_db())\n",
    "vrs_mappings_dict = {}\n",
    "scores_dict_coding = {}\n",
    "mavedb_ids_coding = {}\n",
    "\n",
    "i = 10\n",
    "\n",
    "item = mappings_dict[dat.at[i, 'urn']]\n",
    "#if blat_check(i) == False:\n",
    "    #   vrs_mappings_dict[dat.at[i, 'urn']] = 'BLAT hit not found on correct chromosome'\n",
    "    #  continue\n",
    "# get scoreset for this urn from mavedb\n",
    "string = 'https://api.mavedb.org/api/v1/score-sets/' + mave_dat.at[i, 'urn']+ '/scores'\n",
    "origdat = requests.get(string).content\n",
    "vardat = pd.read_csv(io.StringIO(origdat.decode('utf-8')))\n",
    "scores = vardat['score'].to_list()\n",
    "accessions = vardat['accession'].to_list()\n",
    "\n",
    "mappings_list = []\n",
    "scores_list = []\n",
    "accessions_list = []\n",
    "\n",
    "item = mave_blat_dict[dat.at[i, 'urn']]\n",
    "ranges = get_locs_list(item['hits'])\n",
    "hits = get_hits_list(item['hits'])\n",
    "ref = get_chr(dp, item['chrom'])\n",
    "ts = dat.at[i, 'target_sequence']\n",
    "strand = mave_blat_dict[dat.at[i, 'urn']]['strand']\n",
    "\n",
    "digest = 'SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "alias_dict_list = [{'namespace': 'ga4gh', 'alias': digest}]\n",
    "sr.store(ts, nsaliases = alias_dict_list) # Add custom digest to SeqRepo\n",
    "\n",
    "ntlist = vardat['hgvs_nt']\n",
    "varm = vardat['hgvs_pro']\n",
    "sn = []\n",
    "accn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f8c1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_haplotype_allele_temp(var, ref, offset, l, tr, dp, ts, mapped, ranges, hits, strand):\n",
    "    var = var.lstrip(f'{l}.')\n",
    "    if '[' in var:\n",
    "        var = var[1:][:-1]\n",
    "        varlist = var.split(';')\n",
    "        varlist = list(set(varlist))\n",
    "    else:\n",
    "        varlist = list()\n",
    "        varlist.append(var)\n",
    "\n",
    "    locs = {}\n",
    "    alleles = []\n",
    "\n",
    "    for i in range(len(varlist)):\n",
    "        hgvs_string = ref + ':'+ l +'.' + varlist[i]\n",
    "        allele = tr.translate_from(hgvs_string, 'hgvs')\n",
    "        \n",
    "        if mapped == 'pre':\n",
    "            allele.location.sequenceReference.refgetAccession = 'SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "            # dups haven't been tested yet, need to find a test case\n",
    "            if 'dup' in hgvs_string:\n",
    "                print('dup in hgvs string. this has not been tested yet, review output.')\n",
    "                allele.state.sequence.root = 2*str(sr[str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "                \n",
    "        else:\n",
    "            if l != 'g':\n",
    "                allele.location.start = allele.location.start + offset\n",
    "                allele.location.end = allele.location.end + offset\n",
    "                # dups haven't been fixed yet, need to find a test case\n",
    "                if 'dup' in hgvs_string:\n",
    "                    # not sure if this needs to be allele.state.sequence.root\n",
    "                    print('dup in hgvs string. this has not been tested yet, review output.')\n",
    "                    allele.state.sequence.root = 2*str(sr[str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "                    \n",
    "            else:\n",
    "                start = allele.location.start\n",
    "                if len(hits) == 1 and strand == 1:\n",
    "                    i = 0\n",
    "                    diff = start - hits[i][0]\n",
    "                    diff2 = allele.location.end - start\n",
    "                    allele.location.start = ranges[i][0] + diff\n",
    "                    allele.location.end = allele.location.start + diff2\n",
    "                else:\n",
    "                    for i in range(len(hits)):\n",
    "                        if start >= hits[i][0] and start < hits[i][1]:\n",
    "                            break\n",
    "                    diff = start - hits[i][0]\n",
    "                    diff2 = allele.location.end - start\n",
    "                    if strand == 1: # positive orientation\n",
    "                        allele.location.start = ranges[i][0] + diff\n",
    "                        allele.location.end = allele.location.start + diff2\n",
    "                        # haven't fixed dups yet, need test case\n",
    "                        if 'dup' in hgvs_string:\n",
    "                            print('dup in hgvs string. this has not been tested yet, review output.')\n",
    "                            allele.state.sequence.root = 2*str(sr[\"ga4gh:\" + str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "                    else: \n",
    "                        allele.location.start = ranges[i][1] - diff - diff2\n",
    "                        allele.location.end = allele.location.start + diff2\n",
    "                        # haven't fixed dups yet, need test case\n",
    "                        if 'dup' in hgvs_string:\n",
    "                            print('dup in hgvs string. this has not been tested yet, review output.')\n",
    "                            allele.state.sequence.root = 2*str(sr[str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "                        # haven't tested rev comp yet, need test case\n",
    "                        print('this is a rev comp. this has not been tested yet, review output.')\n",
    "                        allele.state.sequence.root = str(Seq(str(allele.state.sequence.root)).reverse_complement())\n",
    "        \n",
    "        # haven't fixed this if block yet, need test case\n",
    "        # not sure if this needs to be allele.state.sequence.root\n",
    "        if allele.state.sequence.root == 'N' and l != 'p':\n",
    "            print('sequence is N. this has not been tested yet, review output.')\n",
    "            allele.state.sequence.root = str(sr[str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "        print('pre-normalized sequence: ' + allele.state.sequence.root)\n",
    "        print(allele)\n",
    "        allele = normalize(allele, data_proxy = dp)    \n",
    "        print('post-normalized sequence: ' + allele.state.sequence.root)\n",
    "        allele.id = ga4gh_identify(allele)\n",
    "        alleles.append(allele)\n",
    "    \n",
    "    if len(alleles) == 1: # Not haplotype\n",
    "        return alleles[0]\n",
    "    else:\n",
    "        return models.Haplotype(members = alleles)\n",
    "\n",
    "# protein coding\n",
    "#pre\n",
    "#get_haplotype_allele_temp(varm[0], np, 0, 'p', tr, dp, ts, 'pre', '', '', '')\n",
    "#post\n",
    "#get_haplotype_allele_temp(varm[0], np, offset, 'p', tr, dp, ts, 'post', '', '', '')\n",
    "# post, protein coding with nt hgvs column and target seq on rev strand\n",
    "#get_haplotype_allele_temp(ntlist[0][2:], ref, 0, 'g', tr, dp, ts,'post', ranges, hits, strand)\n",
    "    \n",
    "# non protein coding\n",
    "# pre already works\n",
    "# post\n",
    "#get_haplotype_allele_temp(ntlist[0][2:], ref, 0, 'g', tr, dp, ts, 'post', ranges, hits, strand)\n",
    "# variant with 'dup' in hgvs_nt\n",
    "#get_haplotype_allele_temp(ntlist[17][2:], ref, 0, 'g', tr, dp, ts, 'post', ranges, hits, strand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9079ab1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chrom': '17',\n",
       " 'strand': -1,\n",
       " 'target': 'BRCA1 Exon 15',\n",
       " 'target_type': 'Protein coding',\n",
       " 'uniprot': nan,\n",
       " 'coverage': '106 / 106, 100.0',\n",
       " 'identity': 100.0,\n",
       " 'hits':   query_ranges           hit_ranges\n",
       " 0      [0:106]  [43070917:43071023]}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view blat dict for target on rev strand\n",
    "mave_blat_dict['urn:mavedb:00000097-i-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4b6b9fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Post-map VRS translation for protein-coding variants spanning multiple positions has not been implemented.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb Cell 28\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y122sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(parsed_hgvs\u001b[39m.\u001b[39mpositions))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y122sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(parsed_hgvs\u001b[39m.\u001b[39mpositions, mavehgvs\u001b[39m.\u001b[39mposition\u001b[39m.\u001b[39mVariantPosition):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y122sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPost-map VRS translation for protein-coding variants spanning multiple positions has not been implemented.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Post-map VRS translation for protein-coding variants spanning multiple positions has not been implemented."
     ]
    }
   ],
   "source": [
    "parsed_hgvs = mavehgvs.util.parse_variant_strings(['c.78+5_78+10del'])[0][0]\n",
    "print(type(parsed_hgvs.positions))\n",
    "if not isinstance(parsed_hgvs.positions, mavehgvs.position.VariantPosition):\n",
    "    raise NotImplementedError(\"Post-map VRS translation for protein-coding variants spanning multiple positions has not been implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d8275d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_target_id': None, 'variant_count': 1, '_prefix': 'c', '_variant_types': 'sub', '_positions': 106, '_sequences': ('T', 'G')}\n",
      "G\n",
      "106\n",
      "<class 'mavehgvs.position.VariantPosition'>\n",
      "c.106A>C\n"
     ]
    }
   ],
   "source": [
    "import mavehgvs\n",
    "parsed_hgvs = mavehgvs.util.parse_variant_strings(['c.' + '106T>G'])[0][0]\n",
    "print(parsed_hgvs.__dict__)\n",
    "print(parsed_hgvs._sequences[1])\n",
    "print(parsed_hgvs.positions)\n",
    "print(type(parsed_hgvs.positions))\n",
    "#parsed_hgvs._sequences[1] = str(Seq(str(parsed_hgvs._sequences[1])).reverse_complement())\n",
    "if not isinstance(parsed_hgvs.positions, mavehgvs.position.VariantPosition):\n",
    "    raise NotImplementedError(\"Post-map VRS translation for protein-coding variants spanning multiple positions has not been implemented.\")\n",
    "\n",
    "# rev comp each sequence, assuming [0] is original and [1] is variant\n",
    "\n",
    "revcomp_sequences_list = []\n",
    "for sequence in parsed_hgvs._sequences:\n",
    "    revcomp_sequences_list.append(str(Seq(sequence).reverse_complement()))\n",
    "parsed_hgvs._sequences = tuple(revcomp_sequences_list)\n",
    "print(parsed_hgvs)\n",
    "# parsed_hgvs._sequences = tuple(parsed_hgvs._sequences[0], str(Seq(str(parsed_hgvs._sequences[1])).reverse_complement()))\n",
    "# print(parsed_hgvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "85b4154f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021\n",
      "None\n",
      "Glu341\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "parsed_hgvs = mavehgvs.util.parse_variant_strings(['c.1021dup'])[0][0]\n",
    "print(parsed_hgvs.positions)\n",
    "print(parsed_hgvs._sequences)\n",
    "\n",
    "parsed_hgvs = mavehgvs.util.parse_variant_strings(['p.Glu341fs'])[0][0]\n",
    "print(parsed_hgvs.positions)\n",
    "print(parsed_hgvs._sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6ef99803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['urn:mavedb:00000041-a-1', 'urn:mavedb:00000048-a-1', 'urn:mavedb:00000018-a-1', 'urn:mavedb:00000107-a-1', 'urn:mavedb:00000103-d-1', 'urn:mavedb:00000029-a-2', 'urn:mavedb:00000061-b-1', 'urn:mavedb:00000097-q-1', 'urn:mavedb:00000003-a-1', 'urn:mavedb:00000097-i-1', 'urn:mavedb:00000099-a-1'])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mave_blat_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9efe8983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(2, 10), match='Glu341fs'>\n",
      "None\n",
      "(?P<pro_fs>(?P<position>(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))fs)\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "#hgvs_string = ref + ':'+ l +'.' + varlist[i]\n",
    "parsed_hgvs = mavehgvs.util.parse_variant_strings(['p.Glu341fs'])[0][0]\n",
    "import re\n",
    "print(re.search(mavehgvs.patterns.protein.pro_fs, 'p.Glu341fs'))\n",
    "print(re.search(mavehgvs.patterns.protein.pro_fs, 'no'))\n",
    "print(mavehgvs.patterns.protein.pro_fs)\n",
    "\n",
    "if re.search(mavehgvs.patterns.protein.pro_fs, 'p.Glu341fs'):\n",
    "    print('yes')\n",
    "if re.search(mavehgvs.patterns.protein.pro_fs, 'no'):\n",
    "    print('no')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f573fa8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Post-map VRS translation not supported for fs variants denoted with protein hgvs strings",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb Cell 37\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y112sZmlsZQ%3D%3D?line=204'>205</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m models\u001b[39m.\u001b[39mHaplotype(members \u001b[39m=\u001b[39m alleles)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y112sZmlsZQ%3D%3D?line=206'>207</a>\u001b[0m \u001b[39m# protein coding\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y112sZmlsZQ%3D%3D?line=207'>208</a>\u001b[0m \u001b[39m#pre\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y112sZmlsZQ%3D%3D?line=208'>209</a>\u001b[0m \u001b[39m#get_haplotype_allele_temp(varm[0], np, 0, 'p', tr, dp, ts, 'pre', '', '', '')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y112sZmlsZQ%3D%3D?line=257'>258</a>\u001b[0m \u001b[39m# 99-a-1 has fs variants, test those\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y112sZmlsZQ%3D%3D?line=258'>259</a>\u001b[0m \u001b[39m#get_haplotype_allele_sally(varm[17], np, 0, 'p', tr, dp, ts, 'pre', '', '', '')\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y112sZmlsZQ%3D%3D?line=259'>260</a>\u001b[0m get_haplotype_allele_sally(varm[\u001b[39m17\u001b[39;49m], np, offset, \u001b[39m'\u001b[39;49m\u001b[39mp\u001b[39;49m\u001b[39m'\u001b[39;49m, tr, dp, ts, \u001b[39m'\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb Cell 37\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y112sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# TODO protein fs hgvs strings are not supported because they can't be handled by vrs allele translator\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y112sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mif\u001b[39;00m re\u001b[39m.\u001b[39msearch(mavehgvs\u001b[39m.\u001b[39mpatterns\u001b[39m.\u001b[39mprotein\u001b[39m.\u001b[39mpro_fs, \u001b[39mstr\u001b[39m(parsed_hgvs)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y112sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPost-map VRS translation not supported for fs variants denoted with protein hgvs strings\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y112sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m parsed_hgvs\u001b[39m.\u001b[39mpositions\u001b[39m.\u001b[39mposition \u001b[39m=\u001b[39m parsed_hgvs\u001b[39m.\u001b[39mpositions\u001b[39m.\u001b[39mposition \u001b[39m+\u001b[39m offset\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#Y112sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m hgvs_string \u001b[39m=\u001b[39m ref \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(parsed_hgvs)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Post-map VRS translation not supported for fs variants denoted with protein hgvs strings"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import mavehgvs\n",
    "\n",
    "def get_haplotype_allele_sally(var, ref, offset, l, tr, dp, ts, mapped, ranges, hits, strand):\n",
    "    var = var.lstrip(f'{l}.')\n",
    "    if '[' in var:\n",
    "        var = var[1:][:-1]\n",
    "        varlist = var.split(';')\n",
    "        varlist = list(set(varlist))\n",
    "    else:\n",
    "        varlist = list()\n",
    "        varlist.append(var)\n",
    "\n",
    "    locs = {}\n",
    "    alleles = []\n",
    "\n",
    "    for i in range(len(varlist)):\n",
    "        # hgvs_string = ref + ':'+ l +'.' + varlist[i]\n",
    "        # allele = tr.translate_from(hgvs_string, 'hgvs')\n",
    "        \n",
    "        if mapped == 'pre':\n",
    "            hgvs_string = ref + ':'+ l +'.' + varlist[i]\n",
    "\n",
    "            # TODO protein fs hgvs strings are not supported because they can't be handled by vrs allele translator\n",
    "            if re.search(mavehgvs.patterns.protein.pro_fs, hgvs_string):\n",
    "                raise NotImplementedError(\"Pre-map VRS translation not supported for fs variants denoted with protein hgvs strings\")\n",
    "\n",
    "            allele = tr.translate_from(hgvs_string, 'hgvs')\n",
    "            # it's necessary to update the sequence identifier after translation, rather than including it in the hgvs string,\n",
    "            # because the hgvs parser expects a digit after the 'SQ.'\n",
    "            # note: not updating sequence reference until after normalization,\n",
    "            # because computed sequence identifier should include 'ga4gh:SQ', (see example here https://vrs.ga4gh.org/en/1.1/impl-guide/example.html)\n",
    "            # and the 'ga4gh:' breaks the normalizer\n",
    "            #allele.location.sequenceReference.refgetAccession = 'SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "\n",
    "            # dups haven't been tested yet, need to find a test case\n",
    "            if 'dup' in hgvs_string:\n",
    "                print('dup in hgvs string. this has not been tested yet, review output.')\n",
    "                allele.state.sequence.root = 2*str(sr[str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "                \n",
    "        else:\n",
    "            if l != 'g':\n",
    "                # TODO do we need to do anything for negative strand if using p. hgvs?\n",
    "                # expecting protein-based ref, so hgvs string is already mostly correct - just need to calculate offset\n",
    "                # could parse whole list outside of for loop since this function takes a list\n",
    "                parsed_hgvs = mavehgvs.util.parse_variant_strings(['p.' + varlist[i]])[0][0]\n",
    "                # looks like offset is calculated based on amino acids, so this should be correct, but should validate\n",
    "                # may want to only do this if offset != 0? i guess that depends on how often offset == 0\n",
    "\n",
    "                # TODO positions can be a tuple if there are multiple positions associated with the variant.\n",
    "                # if positions is a tuple, accessing position like this won't work.\n",
    "                # so need to check length of parsed_hgvs.positions\n",
    "                # should we expect multi-position protein variants?\n",
    "                # looks like yes - example from mavehgvs spec: p.His7_Gln8insSer\n",
    "                \n",
    "                if not isinstance(parsed_hgvs.positions, mavehgvs.position.VariantPosition):\n",
    "                    raise NotImplementedError(\"Post-map VRS translation for protein-coding variants spanning multiple positions has not been implemented.\")\n",
    "                \n",
    "                # TODO protein fs hgvs strings are not supported because they can't be handled by vrs allele translator\n",
    "                if re.search(mavehgvs.patterns.protein.pro_fs, str(parsed_hgvs)):\n",
    "                    raise NotImplementedError(\"Post-map VRS translation not supported for fs variants denoted with protein hgvs strings\")\n",
    "                \n",
    "                parsed_hgvs.positions.position = parsed_hgvs.positions.position + offset\n",
    "                hgvs_string = ref + ':' + str(parsed_hgvs)\n",
    "                allele = tr.translate_from(hgvs_string, 'hgvs')\n",
    "\n",
    "                # allele.location.start = allele.location.start + offset\n",
    "                # allele.location.end = allele.location.end + offset\n",
    "                # dups haven't been fixed yet, need to find a test case\n",
    "                if 'dup' in hgvs_string:\n",
    "                    # not sure if this needs to be allele.state.sequence.root\n",
    "                    print('dup in hgvs string. this has not been tested yet, review output.')\n",
    "                    allele.state.sequence.root = 2*str(sr[str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "                    \n",
    "            else:\n",
    "                # TODO not sure if this should be c, n, or g\n",
    "                # works for now but will need to be correct if we want to return the hgvs string (which we probably do)\n",
    "                parsed_hgvs = mavehgvs.util.parse_variant_strings(['c.' + varlist[i]])[0][0]\n",
    "                # start = allele.location.start\n",
    "                if not isinstance(parsed_hgvs.positions, mavehgvs.position.VariantPosition):\n",
    "                    raise NotImplementedError(\"Post-map VRS translation for non-protein-coding variants spanning multiple positions has not been implemented.\")\n",
    "\n",
    "                start = parsed_hgvs.positions.position - 1 #hgvs uses 1-based numbering for c. sequences, while blat hits are 0-based\n",
    "\n",
    "                # get hit\n",
    "                if len(hits) == 1:\n",
    "                    i = 0\n",
    "                else:\n",
    "                    for i in range(len(hits)):\n",
    "                        if start >= hits[i][0] and start < hits[i][1]:\n",
    "                            break\n",
    "\n",
    "                # if hit is on positive strand\n",
    "                if strand == 1:\n",
    "                    # get variant start relative to the reference (the \"hit\")\n",
    "                    # distance from beginning of query to variant start position:\n",
    "                    query_to_start = start - hits[i][0]\n",
    "                    # distance from beginning of ref to the variant start position:\n",
    "                    ref_to_start = ranges[i][0] + query_to_start\n",
    "                    # hgvs is 1-based, so convert back to 1-based\n",
    "                    parsed_hgvs.positions.position = ref_to_start + 1\n",
    "                # if hit is on negative strand    \n",
    "                else:\n",
    "                    # in this case, picture the rev comp of the query/variant as mapping to the positive strand of the ref\n",
    "                    # the start of the reverse complement of the variant is the end of the \"original\" variant\n",
    "                    # so we need to know where the end of the original variant is, relative to the query molecule\n",
    "                    # for single-position variants, we'll assume the end (rev comp view) is equal to: start - 1       \n",
    "                    # TODO this works for single-position variants only!\n",
    "                    # this error is redundant (should be caught above),\n",
    "                    # but since it's not necessarily obvious that this works for\n",
    "                    # single-position variants only,\n",
    "                    # I'm putting it here as well because development\n",
    "                    # will need to happen here as well in order to support multi-position\n",
    "                    # variants, since diff2 = 1 is ONLY a good assumption for single-position variants\n",
    "                    if not isinstance(parsed_hgvs.positions, mavehgvs.position.VariantPosition):\n",
    "                        raise NotImplementedError(\"Post-map VRS translation for protein-coding variants spanning multiple positions has not been implemented.\")\n",
    "                    \n",
    "                    # the distance between the start and end of the variant is dependent on the number of positions covered by the variant!\n",
    "                    # this is hardcoded for single-position variants, for now\n",
    "                    end = start\n",
    "                    # subtract 1 from end of hit range, because blat ranges are 0-based [start, end)\n",
    "                    ref_to_start = (ranges[i][1] -1 ) - (end - hits[i][0])\n",
    "                    # or could do ranges[i][0] + (end - hits[i][1]), is one better than the other? any cases where one might be inaccurate?\n",
    "                    # hgvs is 1-based, so convert back to 1-based\n",
    "                    parsed_hgvs.positions.position = ref_to_start + 1\n",
    "\n",
    "                    # rev comp each sequence, assuming [0] is original and [1] is variant\n",
    "                    # this is only tested for single position variants\n",
    "\n",
    "                    revcomp_sequences_list = []\n",
    "                    for sequence in parsed_hgvs._sequences:\n",
    "                        revcomp_sequences_list.append(str(Seq(sequence).reverse_complement()))\n",
    "                    parsed_hgvs._sequences = tuple(revcomp_sequences_list)\n",
    "\n",
    "                # get hgvs and allele\n",
    "                hgvs_string = ref + ':' + str(parsed_hgvs)\n",
    "                allele = tr.translate_from(hgvs_string, 'hgvs')\n",
    "\n",
    "\n",
    "                # if len(hits) == 1 and strand == 1:\n",
    "                #     i = 0\n",
    "                #     diff = start - hits[i][0]\n",
    "                #     # diff2 = allele.location.end - start\n",
    "                #     parsed_hgvs.positions.position = ranges[i][0] + diff\n",
    "                #     # allele.location.end = allele.location.start + diff2\n",
    "                # else:\n",
    "                #     for i in range(len(hits)):\n",
    "                #         if start >= hits[i][0] and start < hits[i][1]:\n",
    "                #             break\n",
    "                #     diff = start - hits[i][0]\n",
    "                #     # diff2 = allele.location.end - start\n",
    "                #     if strand == 1: # positive orientation\n",
    "                #         # allele.location.start = ranges[i][0] + diff\n",
    "                #         # allele.location.end = allele.location.start + diff2\n",
    "                #         parsed_hgvs.positions.position = ranges[i][0] + diff\n",
    "                #         # haven't fixed dups yet, need test case\n",
    "                #         if 'dup' in hgvs_string:\n",
    "                #             print('dup in hgvs string. this has not been tested yet, review output.')\n",
    "                #             allele.state.sequence.root = 2*str(sr[\"ga4gh:\" + str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "                #     else: # negative strand\n",
    "                #         # TODO this works for single-position variants only!\n",
    "                #         # this error is redundant (should be caught above),\n",
    "                #         # but since it's not necessarily obvious that this works for\n",
    "                #         # singlle-position variants only,\n",
    "                #         # I'm putting it here as well because development\n",
    "                #         # will need to happen here as well in order to support multi-position\n",
    "                #         # variants, since diff2 = 1 is ONLY a good assumption for single-position variants\n",
    "                #         if len(parsed_hgvs.positions) > 1:\n",
    "                #             raise NotImplementedError(\"Post-map VRS translation for non-protein-coding variants spanning multiple positions has not been implemented.\")\n",
    "                #         # if position is only one variant,\n",
    "                #         # assume that diff2 = 1?\n",
    "                        \n",
    "                #         allele.location.start = ranges[i][1] - diff - diff2\n",
    "                #         allele.location.end = allele.location.start + diff2\n",
    "                #         # haven't fixed dups yet, need test case\n",
    "                #         if 'dup' in hgvs_string:\n",
    "                #             print('dup in hgvs string. this has not been tested yet, review output.')\n",
    "                #             allele.state.sequence.root = 2*str(sr[str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "                #         # haven't tested rev comp yet, need test case\n",
    "                #         print('this is a rev comp. this has not been tested yet, review output.')\n",
    "                #         allele.state.sequence.root = str(Seq(str(allele.state.sequence.root)).reverse_complement())\n",
    "        \n",
    "        # TODO dups and 'fs' need to either be corrected after the fact, or use the ref or target sequence to correct them\n",
    "        # this doesn't currently work for dups, definitely won't work for rev comp fs, may work for + strand fs but haven't tested\n",
    "\n",
    "        # haven't fixed this if block yet, need test case\n",
    "        # not sure if this needs to be allele.state.sequence.root\n",
    "        if allele.state.sequence.root == 'N' and l != 'p':\n",
    "            print('sequence is N. this has not been tested yet, review output.')\n",
    "            allele.state.sequence.root = str(sr[str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "        allele = normalize(allele, data_proxy = dp)\n",
    "    \n",
    "        # update sequence reference id after normalization, see commented notes in pre mapping section above\n",
    "        if mapped == 'pre':\n",
    "            # not sure if refgetAccession is the appropriate field to update here, since this is a ga4gh computed seq id.\n",
    "            # do ga4gh computed seq ids count as refget accession ids?\n",
    "            allele.location.sequenceReference.refgetAccession = 'ga4gh:SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "        allele.id = ga4gh_identify(allele)\n",
    "        alleles.append(allele)\n",
    "    \n",
    "    if len(alleles) == 1: # Not haplotype\n",
    "        return alleles[0]\n",
    "    else:\n",
    "        return models.Haplotype(members = alleles)\n",
    "\n",
    "# protein coding\n",
    "#pre\n",
    "#get_haplotype_allele_temp(varm[0], np, 0, 'p', tr, dp, ts, 'pre', '', '', '')\n",
    "#post\n",
    "#get_haplotype_allele_temp(varm[0], np, offset, 'p', tr, dp, ts, 'post', '', '', '')\n",
    "# post, protein coding with nt hgvs column and target seq on rev strand\n",
    "#get_haplotype_allele_temp(ntlist[0][2:], ref, 0, 'g', tr, dp, ts,'post', ranges, hits, strand)\n",
    "    \n",
    "# non protein coding\n",
    "# pre already works\n",
    "# post\n",
    "#get_haplotype_allele_temp(ntlist[0][2:], ref, 0, 'g', tr, dp, ts, 'post', ranges, hits, strand)\n",
    "# variant with 'dup' in hgvs_nt\n",
    "#get_haplotype_allele_temp(ntlist[17][2:], ref, 0, 'g', tr, dp, ts, 'post', ranges, hits, strand)\n",
    "    \n",
    "\n",
    "# sally's changes\n",
    "# protein coding\n",
    "# pre\n",
    "#get_haplotype_allele_sally(varm[0], np, 0, 'p', tr, dp, ts, 'pre', '', '', '')\n",
    "# post\n",
    "# new = get_haplotype_allele_sally(varm[0], np, offset, 'p', tr, dp, ts, 'post', '', '', '')\n",
    "# # compare to old version\n",
    "# old = get_haplotype_allele_temp(varm[0], np, offset, 'p', tr, dp, ts, 'post', '', '', '')\n",
    "# print(\"new\")\n",
    "# print(new)\n",
    "# print()\n",
    "# print(\"old\")\n",
    "# print(old)\n",
    "# pass!\n",
    "\n",
    "# post, rev comp hgvs_nt\n",
    "# old = get_haplotype_allele_temp(ntlist[0][2:], ref, 0, 'g', tr, dp, ts,'post', ranges, hits, strand)\n",
    "# new = get_haplotype_allele_sally(ntlist[0][2:], ref, 0, 'g', tr, dp, ts,'post', ranges, hits, strand)\n",
    "# print(\"new\")\n",
    "# print(new)\n",
    "# print()\n",
    "# print(\"old\")\n",
    "# print(old)\n",
    "# pass!\n",
    "\n",
    "# post, non-rev-comp hgvs_nt\n",
    "# old = get_haplotype_allele_temp(ntlist[0][2:], ref, 0, 'g', tr, dp, ts, 'post', ranges, hits, strand)\n",
    "# new = get_haplotype_allele_sally(ntlist[0][2:], ref, 0, 'g', tr, dp, ts, 'post', ranges, hits, strand)\n",
    "# print(\"new\")\n",
    "# print(new)\n",
    "# print()\n",
    "# print(\"old\")\n",
    "# print(old)\n",
    "# pass!\n",
    "    \n",
    "# 99-a-1 has fs variants, test those\n",
    "#get_haplotype_allele_sally(varm[17], np, 0, 'p', tr, dp, ts, 'pre', '', '', '')\n",
    "get_haplotype_allele_sally(varm[17], np, offset, 'p', tr, dp, ts, 'post', '', '', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e0274f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106T>G\n",
      "  query_ranges           hit_ranges\n",
      "0      [0:106]  [43070917:43071023]\n",
      "[[43070917, 43071023]]\n",
      "[[0, 106]]\n"
     ]
    }
   ],
   "source": [
    "print(ntlist[0][2:])\n",
    "print(mave_blat_dict[dat.at[9, 'urn']]['hits'])\n",
    "print(ranges)\n",
    "print(hits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "08803c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "parsed = mavehgvs.util.parse_variant_strings(['p.Glu341fs'])[0][0]\n",
    "print(parsed.positions.position)\n",
    "print(parsed._sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4b992e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(His7, Gln8)\n",
      "7\n",
      "8\n",
      "Ser\n"
     ]
    }
   ],
   "source": [
    "# mavehgvs position test\n",
    "parsed = mavehgvs.util.parse_variant_strings(['p.His7_Gln8insSer'])[0][0]\n",
    "print(parsed.positions)\n",
    "print(parsed.positions[0].position)\n",
    "# is it a safe assumption that mavehgvs variant positions are always ordered least to greatest by int?\n",
    "print(parsed.positions[-1].position)\n",
    "\n",
    "print(parsed._sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69bfdf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  query_ranges           hit_ranges\n",
      "0       [0:52]  [37397802:37397854]\n",
      "1     [52:232]  [37400114:37400294]\n",
      "2    [232:309]  [37401601:37401678]\n",
      "3    [309:463]  [37402434:37402588]\n",
      "4    [463:595]  [37402748:37402880]\n",
      "5    [595:750]  [37403170:37403325]\n"
     ]
    }
   ],
   "source": [
    "# how do blat output ranges work?\n",
    "print(mave_blat_dict[dat.at[0, 'urn']]['hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f65a95b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "(?P<pro_multi>p\\.\\[(?:(?:(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))?(?:=))|(?:\\(=\\)))|(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)))|(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))fs)|(?:(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))_(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))del)|(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))del))|(?:(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))_(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))dup)|(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))dup))|(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))_(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))ins(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)+))|(?:(?:(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))_(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*)))|(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*)))delins(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)+)))(?:;(?:(?:(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))?(?:=))|(?:\\(=\\)))|(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)))|(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))fs)|(?:(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))_(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))del)|(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))del))|(?:(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))_(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))dup)|(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))dup))|(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))_(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))ins(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)+))|(?:(?:(?:(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*))_(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*)))|(?:(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)[1-9][0-9]*)))delins(?:(?:Ala|Arg|Asn|Asp|Cys|Gln|Glu|Gly|His|Ile|Leu|Lys|Met|Phe|Pro|Ser|Thr|Trp|Tyr|Val|Ter)+)))){1,}\\])\n",
      "<re.Match object; span=(0, 18), match='p.Val137_Pro142del'>\n"
     ]
    }
   ],
   "source": [
    "print(re.search(mavehgvs.patterns.protein.pro_multi_variant, 'p.Val137_Pro142del'))\n",
    "print(mavehgvs.patterns.protein.pro_multi_variant)\n",
    "print(re.search(mavehgvs.patterns.protein.pro_single_variant, 'p.Val137_Pro142del'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3a13aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_haplotype_allele_mavehgvs(var, ref, offset, l, tr, dp, ts, mapped, ranges, hits, strand):\n",
    "    var = var.lstrip(f'{l}.')\n",
    "    if '[' in var:\n",
    "        var = var[1:][:-1]\n",
    "        varlist = var.split(';')\n",
    "        varlist = list(set(varlist))\n",
    "    else:\n",
    "        varlist = list()\n",
    "        varlist.append(var)\n",
    "\n",
    "    locs = {}\n",
    "    alleles = []\n",
    "\n",
    "    for i in range(len(varlist)):\n",
    "        # hgvs_string = ref + ':'+ l +'.' + varlist[i]\n",
    "        # allele = tr.translate_from(hgvs_string, 'hgvs')\n",
    "        \n",
    "        if mapped == 'pre':\n",
    "            hgvs_string = ref + ':'+ l +'.' + varlist[i]\n",
    "\n",
    "            # TODO protein fs hgvs strings are not supported because they can't be handled by vrs allele translator\n",
    "            if re.search(mavehgvs.patterns.protein.pro_fs, hgvs_string):\n",
    "                raise NotImplementedError(\"Pre-map VRS translation not supported for fs variants denoted with protein hgvs strings\")\n",
    "            \n",
    "            # TODO multi position variants\n",
    "            # this actually works for pre-map, but don't support it until post-map works\n",
    "            if re.search(mavehgvs.patterns.protein.pro_multi_variant, hgvs_string):\n",
    "                raise NotImplementedError(\"Pre-map VRS translation not supported for multi-position variants\")\n",
    "\n",
    "            allele = tr.translate_from(hgvs_string, 'hgvs')\n",
    "            # it's necessary to update the sequence identifier after translation, rather than including it in the hgvs string,\n",
    "            # because the hgvs parser expects a digit after the 'SQ.'\n",
    "            # note: not updating sequence reference until after normalization,\n",
    "            # because computed sequence identifier should include 'ga4gh:SQ', (see example here https://vrs.ga4gh.org/en/1.1/impl-guide/example.html)\n",
    "            # and the 'ga4gh:' breaks the normalizer\n",
    "            #allele.location.sequenceReference.refgetAccession = 'SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "\n",
    "            if 'dup' in hgvs_string:\n",
    "                print('dup in hgvs string. this has not been tested yet, review output.')\n",
    "                allele.state.sequence.root = 2*str(sr[str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "                \n",
    "        else:\n",
    "            if l != 'g':\n",
    "                # TODO do we need to do anything for negative strand if using p. hgvs?\n",
    "                # expecting protein-based ref, so hgvs string is already mostly correct - just need to calculate offset\n",
    "                # could parse whole list outside of for loop since this function takes a list\n",
    "                parsed_hgvs = mavehgvs.util.parse_variant_strings(['p.' + varlist[i]])[0][0]\n",
    "                # looks like offset is calculated based on amino acids, so this should be correct, but should validate\n",
    "                # may want to only do this if offset != 0? i guess that depends on how often offset == 0\n",
    "\n",
    "                # TODO positions can be a tuple if there are multiple positions associated with the variant.\n",
    "                # if positions is a tuple, accessing position like this won't work.\n",
    "                # so need to check length of parsed_hgvs.positions\n",
    "                # should we expect multi-position protein variants?\n",
    "                # looks like yes - example from mavehgvs spec: p.His7_Gln8insSer\n",
    "                \n",
    "                if not isinstance(parsed_hgvs.positions, mavehgvs.position.VariantPosition):\n",
    "                    raise NotImplementedError(\"Post-map VRS translation for protein-coding variants spanning multiple positions has not been implemented.\")\n",
    "                \n",
    "                # TODO protein fs hgvs strings are not supported because they can't be handled by vrs allele translator\n",
    "                if re.search(mavehgvs.patterns.protein.pro_fs, str(parsed_hgvs)):\n",
    "                    raise NotImplementedError(\"Post-map VRS translation not supported for fs variants denoted with protein hgvs strings\")\n",
    "\n",
    "                parsed_hgvs.positions.position = parsed_hgvs.positions.position + offset\n",
    "                hgvs_string = ref + ':' + str(parsed_hgvs)\n",
    "                allele = tr.translate_from(hgvs_string, 'hgvs')\n",
    "\n",
    "                # allele.location.start = allele.location.start + offset\n",
    "                # allele.location.end = allele.location.end + offset\n",
    "                # dups haven't been fixed yet, need to find a test case\n",
    "                if 'dup' in hgvs_string:\n",
    "                    # not sure if this needs to be allele.state.sequence.root\n",
    "                    print('dup in hgvs string. this has not been tested yet, review output.')\n",
    "                    allele.state.sequence.root = 2*str(sr[str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "                    \n",
    "            else:\n",
    "                # can we assume that the noncoding hgvs strings coming in from mavedb in the hgvs_nt column are c.?\n",
    "                parsed_hgvs = mavehgvs.util.parse_variant_strings(['c.' + varlist[i]])[0][0]\n",
    "                # start = allele.location.start\n",
    "                if not isinstance(parsed_hgvs.positions, mavehgvs.position.VariantPosition):\n",
    "                    raise NotImplementedError(\"Post-map VRS translation for non-protein-coding variants spanning multiple positions has not been implemented.\")\n",
    "                \n",
    "                start = parsed_hgvs.positions.position - 1 #hgvs uses 1-based numbering for c. sequences, while blat hits are 0-based\n",
    "\n",
    "                # get hit\n",
    "                if len(hits) == 1:\n",
    "                    i = 0\n",
    "                else:\n",
    "                    for i in range(len(hits)):\n",
    "                        if start >= hits[i][0] and start < hits[i][1]:\n",
    "                            break\n",
    "\n",
    "                # if hit is on positive strand\n",
    "                if strand == 1:\n",
    "                    # get variant start relative to the reference (the \"hit\")\n",
    "                    # distance from beginning of query to variant start position:\n",
    "                    query_to_start = start - hits[i][0]\n",
    "                    # distance from beginning of ref to the variant start position:\n",
    "                    ref_to_start = ranges[i][0] + query_to_start\n",
    "                    # hgvs is 1-based, so convert back to 1-based\n",
    "                    parsed_hgvs.positions.position = ref_to_start + 1\n",
    "                # if hit is on negative strand    \n",
    "                else:\n",
    "                    # in this case, picture the rev comp of the query/variant as mapping to the positive strand of the ref\n",
    "                    # the start of the reverse complement of the variant is the end of the \"original\" variant\n",
    "                    # so we need to know where the end of the original variant is, relative to the query molecule\n",
    "                    # for single-position variants, we'll assume the end (rev comp view) is equal to: start - 1       \n",
    "                    # TODO this works for single-position variants only!\n",
    "                    # this error is redundant (should be caught above),\n",
    "                    # but since it's not necessarily obvious that this works for\n",
    "                    # single-position variants only,\n",
    "                    # I'm putting it here as well because development\n",
    "                    # will need to happen here as well in order to support multi-position\n",
    "                    # variants, since diff2 = 1 is ONLY a good assumption for single-position variants\n",
    "                    if not isinstance(parsed_hgvs.positions, mavehgvs.position.VariantPosition):\n",
    "                        raise NotImplementedError(\"Post-map VRS translation for protein-coding variants spanning multiple positions has not been implemented.\")\n",
    "                    \n",
    "                    # the distance between the start and end of the variant is dependent on the number of positions covered by the variant!\n",
    "                    # this is hardcoded for single-position variants, for now\n",
    "                    end = start\n",
    "                    # subtract 1 from end of hit range, because blat ranges are 0-based [start, end)\n",
    "                    ref_to_start = (ranges[i][1] -1 ) - (end - hits[i][0])\n",
    "                    # or could do ranges[i][0] + (end - hits[i][1]), is one better than the other? any cases where one might be inaccurate?\n",
    "                    # hgvs is 1-based, so convert back to 1-based\n",
    "                    parsed_hgvs.positions.position = ref_to_start + 1\n",
    "\n",
    "                    # rev comp each sequence, assuming [0] is original and [1] is variant\n",
    "                    # this is only tested for single position variants\n",
    "\n",
    "                    revcomp_sequences_list = []\n",
    "                    for sequence in parsed_hgvs._sequences:\n",
    "                        revcomp_sequences_list.append(str(Seq(sequence).reverse_complement()))\n",
    "                    parsed_hgvs._sequences = tuple(revcomp_sequences_list)\n",
    "\n",
    "                # get hgvs and allele\n",
    "                hgvs_string = ref + ':' + str(parsed_hgvs)\n",
    "                allele = tr.translate_from(hgvs_string, 'hgvs')\n",
    "        \n",
    "        # TODO dups will need to be corrected after the allele object is created, because the mavehgvs string\n",
    "        # does not contain information about the identity of the base that is duplicated\n",
    "        # not immediately sure how to handle rev comp dups\n",
    "\n",
    "        # haven't fixed this if block yet, need test case\n",
    "        # not sure if this needs to be allele.state.sequence.root\n",
    "        if allele.state.sequence.root == 'N' and l != 'p':\n",
    "            print('sequence is N. this has not been tested yet, review output.')\n",
    "            allele.state.sequence.root = str(sr[str(allele.location.sequenceReference.refgetAccession)][allele.location.start:allele.location.end])\n",
    "        allele = normalize(allele, data_proxy = dp)\n",
    "    \n",
    "        # update sequence reference id after normalization, see commented notes in pre mapping section above\n",
    "        if mapped == 'pre':\n",
    "            # not sure if refgetAccession is the appropriate field to update here, since this is a ga4gh computed seq id.\n",
    "            # do ga4gh computed seq ids count as refget accession ids?\n",
    "            allele.location.sequenceReference.refgetAccession = 'ga4gh:SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "        allele.id = ga4gh_identify(allele)\n",
    "        alleles.append(allele)\n",
    "    \n",
    "    if len(alleles) == 1: # Not haplotype\n",
    "        return alleles[0]\n",
    "    else:\n",
    "        return models.Haplotype(members = alleles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3d361bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urn:mavedb:00000099-a-1\n",
      "skipping, pre-map fs\n",
      "p.Glu341fs\n",
      "skipping, pre-map fs\n",
      "p.Phe13fs\n",
      "skipping, post-map multi position variant\n",
      "p.Val137_Pro142del\n",
      "skipping, pre-map fs\n",
      "p.Leu328fs\n",
      "skipping, pre-map fs\n",
      "p.Asn315fs\n",
      "skipping, pre-map fs\n",
      "p.Ser334fs\n",
      "skipping, pre-map fs\n",
      "p.Ala335fs\n",
      "skipping, post-map multi position variant\n",
      "p.Tyr206_Phe208del\n",
      "skipping, pre-map fs\n",
      "p.Thr340fs\n",
      "skipping, pre-map fs\n",
      "p.Glu341fs\n",
      "skipping, pre-map fs\n",
      "p.Glu332fs\n",
      "skipping, pre-map fs\n",
      "p.Pro327fs\n",
      "skipping, post-map multi position variant\n",
      "p.Arg69_Leu72del\n",
      "skipping, pre-map fs\n",
      "p.Pro327fs\n",
      "skipping, post-map multi position variant\n",
      "p.Leu318_Thr319delinsPro\n",
      "skipping, pre-map fs\n",
      "p.Ter349fs\n",
      "skipping, pre-map fs\n",
      "p.Ter349fs\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb Cell 43\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X35sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m             \u001b[39mprint\u001b[39m(varm[j])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X35sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X35sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m tempdat \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m'\u001b[39;49m\u001b[39mpre_mapping\u001b[39;49m\u001b[39m'\u001b[39;49m: var_ids_pre_map, \u001b[39m'\u001b[39;49m\u001b[39mmapped\u001b[39;49m\u001b[39m'\u001b[39;49m: var_ids_post_map})\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X35sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m mappings_list\u001b[39m.\u001b[39mappend(tempdat)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X35sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m scores_list\u001b[39m.\u001b[39mappend(spro)\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    761\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    762\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    763\u001b[0m     )\n\u001b[1;32m    765\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    766\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    768\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    769\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# VRS Variant Mapping - Coding Scoresets\n",
    "dp = SeqRepoDataProxy(sr = sr)\n",
    "tr = AlleleTranslator(data_proxy = dp, normalize = False)\n",
    "qh = QueryHandler(create_db())\n",
    "vrs_mappings_dict = {}\n",
    "scores_dict_coding = {}\n",
    "mavedb_ids_coding = {}\n",
    "\n",
    "mave_dat = pd.read_csv('mave_dat.csv')\n",
    "dat = mave_dat\n",
    "\n",
    "# for each urn in the mave data requested from mavedb:\n",
    "#for i in range(len(dat.index)):\n",
    "for i in range(0,1):\n",
    "    i = 10\n",
    "    # this section only processes protein coding sequences\n",
    "    if dat.at[i, 'target_type'] == 'Protein coding' or dat.at[i, 'target_type'] == 'protein_coding':\n",
    "        # if there is a mapping entry for this urn:\n",
    "        if dat.at[i, 'urn'] in mappings_dict.keys():\n",
    "            print(dat.at[i, 'urn'])\n",
    "            # grab the urn's mapping entry\n",
    "            item = mappings_dict[dat.at[i, 'urn']]\n",
    "            # get scoreset for this urn from mavedb\n",
    "            string = 'https://api.mavedb.org/api/v1/score-sets/' + mave_dat.at[i, 'urn']+ '/scores'\n",
    "            origdat = requests.get(string).content\n",
    "            vardat = pd.read_csv(io.StringIO(origdat.decode('utf-8')))\n",
    "            scores = vardat['score'].to_list()\n",
    "            accessions = vardat['accession'].to_list()\n",
    "            \n",
    "            mappings_list = []\n",
    "            scores_list = []\n",
    "            accessions_list = []\n",
    "        \n",
    "            # Process protein column\n",
    "            var_ids_pre_map = []\n",
    "            var_ids_post_map = []\n",
    "            \n",
    "            if len(item) != 0:\n",
    "                np = item[0]\n",
    "                offset = item[1]\n",
    "            varm = vardat['hgvs_pro']\n",
    "        \n",
    "            ts = dat.at[i, 'target_sequence']\n",
    "            if len(set(str(ts))) > 4:\n",
    "                stri = str(ts)\n",
    "            \n",
    "            else:\n",
    "                ts = Seq(ts)\n",
    "                ts = str(ts.translate(table=1)).replace('*', '')\n",
    "                \n",
    "            digest = 'SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "            alias_dict_list = [{'namespace': 'ga4gh', 'alias': digest}]\n",
    "            sr.store(ts, nsaliases = alias_dict_list) # Add custom digest to SeqRepo\n",
    "            \n",
    "            spro = []\n",
    "            accpro = []\n",
    "            \n",
    "            for j in range(len(varm)):\n",
    "                if type(varm[j]) != str or len(varm[j]) == 3 or varm[j] == '_wt' or varm[j] == '_sy':\n",
    "                    continue\n",
    "                if varm[j].startswith('NP') == True:\n",
    "                    var_ids_pre_map.append(tr.translate_from(varm[j], 'hgvs'))\n",
    "                    var_ids_post_map.append(tr.translate_from(varm[j], 'hgvs'))\n",
    "                    spro.append(scores[j])\n",
    "                    accpro.append(accessions[j])\n",
    "                else:\n",
    "                    try:\n",
    "                        if np.startswith('N') == True:\n",
    "                            var_ids_pre_map.append(get_haplotype_allele_mavehgvs(varm[j], np, 0, 'p', tr, dp, ts, 'pre', '', '', ''))\n",
    "                            var_ids_post_map.append(get_haplotype_allele_mavehgvs(varm[j], np, offset, 'p', tr, dp, ts, 'post', '', '', ''))\n",
    "                            spro.append(scores[j])\n",
    "                            accpro.append(accessions[j])\n",
    "                        else:\n",
    "                            var_ids_pre_map.append(get_haplotype_allele_mavehgvs(varm[j], np, 0, 'p', tr, dp, ts, 'pre', '', '', ''))\n",
    "                            # TODO ranges and hits don't actually get used by get_haplotype_allele, are they intended to be used here?\n",
    "                            # what is the 'np' that we're expecting here if it doesn't start with 'N'?\n",
    "                            var_ids_post_map.append(get_haplotype_allele_mavehgvs(varm[j], np, offset, 'p', tr, dp, ts, 'post', ranges, hits, ''))\n",
    "                            spro.append(scores[j])\n",
    "                            accpro.append(accessions[j])\n",
    "                    except:\n",
    "                        continue\n",
    "                    \n",
    "            tempdat = pd.DataFrame({'pre_mapping': var_ids_pre_map, 'mapped': var_ids_post_map})\n",
    "            mappings_list.append(tempdat)\n",
    "            scores_list.append(spro)\n",
    "            accessions_list.append(accpro)\n",
    "            \n",
    "            # Process nt column if data present\n",
    "            if vardat['hgvs_nt'].isnull().values.all() == False and '97' not in dat.at[i, 'urn']:\n",
    "                var_ids_pre_map = []\n",
    "                var_ids_post_map = []\n",
    "            \n",
    "                item = mave_blat_dict[dat.at[i, 'urn']]\n",
    "                ranges = get_locs_list(item['hits'])\n",
    "                hits = get_hits_list(item['hits'])\n",
    "                ref = get_chr(dp, item['chrom'])\n",
    "                ts = dat.at[i, 'target_sequence']\n",
    "                strand = mave_blat_dict[dat.at[i, 'urn']]['strand']\n",
    "                \n",
    "                digest = 'SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "                alias_dict_list = [{'namespace': 'ga4gh', 'alias': digest}]\n",
    "                sr.store(ts, nsaliases = alias_dict_list) # Add custom digest to SeqRepo\n",
    "            \n",
    "                ntlist = vardat['hgvs_nt']\n",
    "                varm = vardat['hgvs_pro']\n",
    "                sn = []\n",
    "                accn = []\n",
    "            \n",
    "                for j in range(len(ntlist)):\n",
    "                    if type(ntlist[j]) != str or ntlist[j] == '_wt' or ntlist[j] == '_sy':\n",
    "                        continue\n",
    "                    else:\n",
    "                        try:\n",
    "                            var_ids_pre_map.append(get_haplotype_allele_mavehgvs(ntlist[j][2:], ref, 0, 'g', tr, dp, ts,'pre', ranges, hits, strand).as_dict())\n",
    "                            var_ids_post_map.append(get_haplotype_allele_mavehgvs(ntlist[j][2:], ref, 0, 'g', tr, dp, ts,'post', ranges, hits, strand).as_dict())\n",
    "                            sn.append(scores[j])\n",
    "                            accn.append(accessions[j])\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                tempdat = pd.DataFrame({'pre_mapping': var_ids_pre_map, 'mapped': var_ids_post_map})\n",
    "                mappings_list.append(tempdat)\n",
    "                scores_list.append(sn)\n",
    "                accessions_list.append(accn)\n",
    "            \n",
    "        vrs_mappings_dict[dat.at[i, 'urn']] = mappings_list\n",
    "        scores_dict_coding[dat.at[i, 'urn']] = scores_list\n",
    "        mavedb_ids_coding[dat.at[i, 'urn']] = accessions_list\n",
    "vrs_mappings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad893d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urn:mavedb:00000018-a-1\n",
      "index: 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb Cell 25\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X36sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m         \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X36sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X36sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m tempdat \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m'\u001b[39;49m\u001b[39mpre_mapping\u001b[39;49m\u001b[39m'\u001b[39;49m: var_ids_pre_map, \u001b[39m'\u001b[39;49m\u001b[39mmapped\u001b[39;49m\u001b[39m'\u001b[39;49m: var_ids_post_map})\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X36sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m vrs_noncoding_mappings_dict[dat\u001b[39m.\u001b[39mat[i, \u001b[39m'\u001b[39m\u001b[39murn\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m tempdat\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X36sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m scores_dict_noncoding[dat\u001b[39m.\u001b[39mat[i, \u001b[39m'\u001b[39m\u001b[39murn\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m scores_list\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    761\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    762\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    763\u001b[0m     )\n\u001b[1;32m    765\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    766\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    768\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    769\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/workspace/varianteffect/dcd_mapping/.venv/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# VRS variant mapping non-protein coding scoresets\n",
    "dp = SeqRepoDataProxy(sr = sr)\n",
    "tr = AlleleTranslator(data_proxy = dp, normalize = False)\n",
    "qh = QueryHandler(create_db())\n",
    "vrs_noncoding_mappings_dict = {}\n",
    "scores_dict_noncoding = {}\n",
    "mavedb_ids_noncoding = {}\n",
    "\n",
    "mave_dat = pd.read_csv('mave_dat.csv')\n",
    "dat = mave_dat\n",
    "\n",
    "for i in range(len(dat.index)):\n",
    "    if dat.at[i, 'target_type'] != 'Protein coding' and dat.at[i, 'target_type'] != 'protein_coding':\n",
    "        print(dat.at[i, 'urn'])\n",
    "        item = mave_blat_dict[dat.at[i, 'urn']]\n",
    "        #if blat_check(i) == False:\n",
    "         #   vrs_noncoding_mappings_dict[dat.at[i, 'urn']] = 'BLAT hit not found on correct chromosome'\n",
    "          #  continue\n",
    "        #ranges = get_locs_list(item['hits'])[0]\n",
    "        string = string = 'https://api.mavedb.org/api/v1/score-sets/' + mave_dat.at[i, 'urn']+ '/scores'\n",
    "        origdat = requests.get(string).content\n",
    "        varsdat = pd.read_csv(io.StringIO(origdat.decode('utf-8')))\n",
    "        ntlist = varsdat['hgvs_nt'].to_list()\n",
    "    \n",
    "        var_ids_pre_map = []\n",
    "        var_ids_post_map = []\n",
    "        ranges = get_locs_list(item['hits'])\n",
    "        ref = get_chr(dp, item['chrom'])\n",
    "        hits = get_hits_list(item['hits'])\n",
    "        strand = mave_blat_dict[dat.at[i, 'urn']]['strand']\n",
    "        \n",
    "        ts = dat.at[i, 'target_sequence']\n",
    "        digest = 'SQ.' + sha512t24u(ts.encode('ascii'))\n",
    "        alias_dict_list = [{'namespace': 'ga4gh', 'alias': digest}]\n",
    "        sr.store(ts, nsaliases = alias_dict_list) # Add custom digest to SeqRepo\n",
    "        \n",
    "        scores = varsdat['score'].to_list()\n",
    "        scores_list = []\n",
    "        accessions = varsdat['accession'].to_list()\n",
    "        accessions_list = []\n",
    "\n",
    "        for j in range(len(ntlist)):\n",
    "            if ntlist[j] == '_wt' or ntlist[j] == '_sy':\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    var_ids_pre_map.append(get_haplotype_allele_temp(ntlist[j][2:], ref, 0, 'g', tr, dp, ts, 'pre', ranges, hits, strand))\n",
    "                    var_ids_post_map.append(get_haplotype_allele_temp(ntlist[j][2:], ref, 0, 'g', tr, dp, ts, 'post', ranges, hits, strand))\n",
    "                    scores_list.append(scores[j])\n",
    "                    accessions_list.append(accessions[j])\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        tempdat = pd.DataFrame({'pre_mapping': var_ids_pre_map, 'mapped': var_ids_post_map})\n",
    "        vrs_noncoding_mappings_dict[dat.at[i, 'urn']] = tempdat\n",
    "        scores_dict_noncoding[dat.at[i, 'urn']] = scores_list\n",
    "        mavedb_ids_noncoding[dat.at[i, 'urn']] = accessions_list\n",
    "\n",
    "vrs_noncoding_mappings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c0cbec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vrs_mappings_coding_normalize_false.pickle', 'wb') as fn:\n",
    "    pickle.dump(vrs_mappings_dict, fn, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4432a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scores_coding.pickle', 'wb') as fn:\n",
    "    pickle.dump(scores_dict_coding, fn, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2faf010",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vrs_mappings_noncoding_normalize_false.pickle', 'wb') as fn:\n",
    "    pickle.dump(vrs_noncoding_mappings_dict, fn, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2daea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scores_noncoding.pickle', 'wb') as fn:\n",
    "    pickle.dump(scores_dict_noncoding, fn, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23529dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mavedb_ids_coding.pickle', 'wb') as fn:\n",
    "    pickle.dump(mavedb_ids_coding, fn, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1e5040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mavedb_ids_noncoding.pickle', 'wb') as fn:\n",
    "    pickle.dump(mavedb_ids_noncoding, fn, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "592af332",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'at'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb Cell 29\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Variant Mapping Example - Coding,Noncoding,Protein + Genomic\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ex \u001b[39m=\u001b[39m vrs_mappings_dict[\u001b[39m'\u001b[39m\u001b[39murn:mavedb:00000041-a-1\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(ex\u001b[39m.\u001b[39;49mat[\u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpre_mapping\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(ex\u001b[39m.\u001b[39mat[\u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmapped\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sallybg/workspace/varianteffect/dcd_mapping/notebooks/sally/mavedb_mapping.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ex \u001b[39m=\u001b[39m vrs_noncoding_mappings_dict[\u001b[39m'\u001b[39m\u001b[39murn:mavedb:00000018-a-1\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'at'"
     ]
    }
   ],
   "source": [
    "# Variant Mapping Example - Coding,Noncoding,Protein + Genomic\n",
    "ex = vrs_mappings_dict['urn:mavedb:00000041-a-1'][10]\n",
    "print(ex.at[0, 'pre_mapping'])\n",
    "print(ex.at[0, 'mapped'])\n",
    "\n",
    "ex = vrs_noncoding_mappings_dict['urn:mavedb:00000018-a-1']\n",
    "print(ex.at[0, 'pre_mapping'])\n",
    "print(ex.at[0, 'mapped'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad683a",
   "metadata": {},
   "source": [
    "# The blocks below can be run to access the output in the results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e5ae5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "mave_dat = pd.read_csv('results/mave_dat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "201f3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load alignment data\n",
    "with open('results/mave_blat.pickle', 'rb') as fn:\n",
    "    mave_blat_dict = pickle.load(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18bdefde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mappings data\n",
    "with open('results/mappings.pickle', 'rb') as fn:\n",
    "    mappings_dict = pickle.load(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba2cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coding data\n",
    "with open('results/vrs_mappings_coding_normalize_false.pickle', 'rb') as fn:\n",
    "    vrs_mappings_coding = pickle.load(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea50429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load noncoding data\n",
    "with open('results/vrs_mappings_noncoding_normalize_false.pickle', 'rb') as fn:\n",
    "    vrs_mappings_noncoding = pickle.load(fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
